# LINEトーク履歴解析バックエンド 仕様書

## 1. 概要

本プロジェクトは、LINEのトーク履歴を解析し、1年間の流行語大賞を表示するスマホ用Webアプリケーションのバックエンドです。

### 1.1 主な機能

- LINEトーク履歴ファイル（.txt形式）のアップロードと解析
- 形態素解析による単語の抽出と集計
- メッセージ全文を単語としてカウントする機能
- 集計結果をJSON形式で返すRESTful API
- フロントエンドとのCORS対応

### 1.2 技術スタック

- **言語**: Python 3.11+
- **Webフレームワーク**: FastAPI
- **形態素解析**: MeCab + mecab-python3
- **辞書**: mecab-ipadic-neologd（新語対応）
- **テストフレームワーク**: pytest
- **コンテナ**: Docker + Docker Compose
- **コード品質**: Black, isort, flake8, mypy

---

## 2. システムアーキテクチャ

```
[フロントエンド] ←→ [FastAPI] ←→ [LINEトーク解析エンジン]
                                        ↓
                                   [形態素解析(MeCab)]
```

### 2.1 ディレクトリ構成

```
line_talk_analyzer_backend/
├── app/
│   ├── __init__.py
│   ├── main.py                    # FastAPIアプリケーションのエントリポイント
│   ├── api/
│   │   ├── __init__.py
│   │   ├── v1/
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── analyze.py    # 解析エンドポイント
│   │   │   │   └── health.py     # ヘルスチェック
│   │   │   └── router.py
│   ├── core/
│   │   ├── __init__.py
│   │   ├── config.py              # 設定管理
│   │   └── cors.py                # CORS設定
│   ├── models/
│   │   ├── __init__.py
│   │   ├── request.py             # リクエストモデル
│   │   └── response.py            # レスポンスモデル
│   ├── services/
│   │   ├── __init__.py
│   │   ├── parser.py              # LINEトーク履歴パーサー
│   │   ├── morphological.py      # 形態素解析
│   │   ├── word_counter.py       # 単語カウンター
│   │   └── analyzer.py            # 統合解析サービス
│   └── utils/
│       ├── __init__.py
│       └── validators.py          # バリデーション関数
├── tests/
│   ├── __init__.py
│   ├── conftest.py                # pytest設定
│   ├── fixtures/
│   │   └── sample_talk.txt        # テスト用サンプルデータ
│   ├── unit/
│   │   ├── __init__.py
│   │   ├── test_parser.py
│   │   ├── test_morphological.py
│   │   ├── test_word_counter.py
│   │   └── test_analyzer.py
│   └── integration/
│       ├── __init__.py
│       └── test_api.py
├── docker-compose.yml
├── Dockerfile
├── requirements.txt
├── requirements-ci.txt
├── mypy.ini
├── .dockerignore
├── .gitignore
└── README.md
```

---

## 3. データ形式

### 3.1 入力データ形式（LINEトーク履歴）

```
[LINE] サンプルグループのトーク履歴
保存日時：2024/08/01 00:00

2024/08/01(木)
22:12	hoge山fuga太郎	おはようございます
22:13		piyo田が参加しました。
22:14	piyo田	こんにちは
22:15	hoge山fuga太郎	[スタンプ]
22:16	foo子	よろしくお願いします
```

**フォーマット**:
- ヘッダー行（1行目）: `[LINE] <トーク名>のトーク履歴`
- 保存日時行（2行目）: `保存日時：YYYY/MM/DD HH:MM`
- 空行（3行目）
- 日付行: `YYYY/MM/DD(曜日)`
- メッセージ行: `HH:MM<TAB>ユーザー名<TAB>メッセージ本文`
  - 区切り文字はタブ文字
  - ユーザー名が空の場合もあり（システムメッセージ）
- 画像/スタンプ等: `HH:MM<TAB>ユーザー名<TAB>[スタンプ]` または `HH:MM<TAB>ユーザー名<TAB>[写真]`

### 3.2 APIレスポンス形式

#### 3.2.1 解析結果レスポンス

```json
{
  "status": "success",
  "data": {
    "analysis_period": {
      "start_date": "2024-01-01",
      "end_date": "2024-12-31"
    },
    "total_messages": 1500,
    "total_users": 3,
    "morphological_analysis": {
      "top_words": [
        {
          "word": "おうち",
          "count": 42,
          "part_of_speech": "名詞"
        }
      ]
    },
    "full_message_analysis": {
      "top_messages": [
        {
          "message": "おうち帰りたい",
          "count": 23
        }
      ]
    }
  }
}
```

#### 3.2.2 エラーレスポンス

```json
{
  "status": "error",
  "error": {
    "code": "INVALID_FILE_FORMAT",
    "message": "アップロードされたファイルの形式が無効です"
  }
}
```

---

## 4. API仕様

### 4.1 エンドポイント一覧

| メソッド | パス | 説明 |
|---------|------|------|
| GET | `/api/v1/health` | ヘルスチェック |
| POST | `/api/v1/analyze` | トーク履歴解析 |

### 4.2 詳細仕様

#### 4.2.1 ヘルスチェック

```
GET /api/v1/health
```

**レスポンス**:
```json
{
  "status": "ok",
  "version": "1.0.0"
}
```

#### 4.2.2 トーク履歴解析

```
POST /api/v1/analyze
```

**リクエスト** (multipart/form-data):
- `file`: LINEトーク履歴ファイル（.txt形式）
- `top_n`: 取得する上位単語数（オプション、デフォルト: 50）
- `min_word_length`: 最小単語長（オプション、デフォルト: 1）
- `exclude_parts`: 除外品詞（オプション、カンマ区切り）

**レスポンス**: 3.2.1参照

**エラーコード**:
- `400`: ファイルが指定されていない、ファイル形式が無効
- `413`: ファイルサイズが大きすぎる（上限: 50MB）
- `500`: サーバー内部エラー

---

## 5. 解析処理の詳細

### 5.1 LINEトーク履歴パーサー (`services/parser.py`)

**責務**:
- LINEトーク履歴ファイルを読み込み、構造化データに変換

**処理フロー**:
1. テキストファイルを読み込み、ヘッダーと保存日時行をスキップ
2. 行ごとに読み込み
3. 日付行（`YYYY/MM/DD(曜日)`形式）を検出して現在の日付を更新
4. メッセージ行をタブ文字で分割して解析（時刻、ユーザー名、メッセージ本文）
5. システムメッセージ（ユーザー名が空）、画像（`[写真]`）、スタンプ（`[スタンプ]`）等は除外
6. 構造化データ（リスト）として返却

**データ構造**:
```python
@dataclass
class Message:
    datetime: datetime
    user: str
    content: str
```

### 5.2 形態素解析サービス (`services/morphological.py`)

**責務**:
- MeCabを使用して単語に分解し、品詞情報を付与

**処理フロー**:
1. メッセージ本文をMeCabで形態素解析
2. 品詞情報を抽出
3. 除外品詞（助詞、助動詞など）をフィルタリング
4. 最小単語長でフィルタリング
5. 単語リストを返却

**抽出対象品詞**（デフォルト）:
- 名詞（一般、固有名詞、サ変接続など）
- 動詞（自立）
- 形容詞（自立）
- 副詞
- 感動詞

### 5.3 単語カウンター (`services/word_counter.py`)

**責務**:
- 形態素解析結果とメッセージ全文の集計

#### 5.3.1 形態素解析結果のカウント

1. 各単語の出現回数を集計
2. 単語ごとに出現したメッセージ情報を記録
3. 品詞情報を保持

#### 5.3.2 メッセージ全文のカウント

1. 各メッセージを1単語として扱う
2. 完全一致カウント: 同一メッセージの出現回数
3. 部分一致カウント: そのメッセージを部分文字列として含む他のメッセージ
   - 例: 「それな」を含む「それな；；」をカウント
4. 両方の出現情報を記録

### 5.4 統合解析サービス (`services/analyzer.py`)

**責務**:
- 上記サービスを統合し、API向けのレスポンスを生成

**処理フロー**:
1. パーサーでトーク履歴を構造化
2. 形態素解析で単語抽出
3. 単語カウンターで集計
4. 上位N件を抽出してソート
5. APIレスポンス形式に整形

---

## 6. CORS設定

フロントエンドからのアクセスを許可するため、以下の設定を行う：

- **許可オリジン**: 環境変数 `ALLOWED_ORIGINS` で設定（デフォルト: `["http://localhost:3000"]`）
- **許可メソッド**: `["GET", "POST"]`
- **許可ヘッダー**: `["*"]`
- **クレデンシャル**: `True`

---

## 7. 設定管理

環境変数を使用して設定を管理（`core/config.py`）:

| 変数名 | デフォルト値 | 説明 |
|--------|-------------|------|
| `APP_NAME` | "LINE Talk Analyzer" | アプリケーション名 |
| `APP_VERSION` | "1.0.0" | バージョン |
| `ALLOWED_ORIGINS` | "http://localhost:3000" | CORS許可オリジン（カンマ区切り） |
| `MAX_FILE_SIZE_MB` | 50 | 最大ファイルサイズ（MB） |
| `DEFAULT_TOP_N` | 50 | デフォルト上位取得数 |
| `MIN_WORD_LENGTH` | 1 | 最小単語長 |

---

## 8. 実装計画

**重要**: 各PRの作業が完了したら、以下を必ず実施すること：
1. `/app/doc/PR/PRxx.md`ファイルを作成（xxはPR番号）
   - 実装内容、テスト結果、修正内容などを詳細に記載
   - 既存のPR01.md、PR03.md、PR04.mdを参考にする
2. 本仕様書（SPEC.md）の該当PRのタスクチェックボックスにチェック（`[ ]` → `[x]`）を入れる

### Phase 1: プロジェクト基盤構築（並列実行可能）

#### PR#1: Docker環境セットアップ
**目的**: 開発環境のコンテナ化

**タスク**:
- [x] Dockerfileの作成
  - Python 3.11ベースイメージ
  - MeCabとmecab-ipadic-neologdのインストール
  - 依存パッケージのインストール
- [x] docker-compose.ymlの作成
  - APIサーバーコンテナ定義
  - ポートマッピング（8000:8000）
  - ボリュームマウント
- [x] .dockerignoreの作成
- [x] requirements.txtの更新
  - fastapi
  - uvicorn[standard]
  - mecab-python3
  - python-multipart
- [x] README.mdの更新（環境構築手順）

**テスト計画**:
- コンテナのビルドが成功すること
- コンテナ起動後にMeCabが使用可能なこと
- `docker-compose up`でサーバーが起動すること

**依存**: なし

---

#### PR#2: コード品質ツールのセットアップ
**目的**: コーディング規約の適用

**タスク**:
- [x] mypy.iniの作成（既存があれば確認）
- [x] .vscode/settings.jsonの確認・更新
- [x] .vscode/extensions.jsonの確認・更新
- [x] requirements-ci.txtの確認・更新
  - pytest
  - pytest-cov
  - pytest-asyncio
  - black
  - isort
  - flake8
  - mypy
- [x] .github/workflows/ci.ymlの確認・更新

**テスト計画**:
- 各ツールが正常に動作すること
- サンプルコードでリント・フォーマットが実行されること

**依存**: なし

---

### Phase 2: データ層の実装（並列実行可能）

#### PR#3: データモデル定義
**目的**: APIのリクエスト/レスポンスモデルの定義

**タスク**:
- [x] `app/models/request.py`の実装
  - `AnalyzeRequest`モデル
  - バリデーション定義
- [x] `app/models/response.py`の実装
  - `AnalysisResult`モデル
  - `WordAnalysisResult`モデル
  - `MessageAnalysisResult`モデル
  - `ErrorResponse`モデル
- [x] 型アノテーション完備
- [x] Docstring完備

**テスト計画**:
- 単体テスト: `tests/unit/test_models.py`
  - [x] 各モデルのインスタンス化
  - [x] バリデーションエラーのテスト
  - [x] JSONシリアライズ/デシリアライズのテスト

**依存**: PR#2（コード品質ツール）

---

#### PR#4: LINEトーク履歴パーサーの実装
**目的**: トーク履歴の構造化

**タスク**:
- [x] `app/services/parser.py`の実装
  - `Message`データクラス
  - `LineMessageParser`クラス
    - `parse()`メソッド: ファイルを読み込んで構造化
    - `_parse_date_line()`メソッド: 日付行の解析
    - `_parse_message_line()`メソッド: メッセージ行の解析
- [x] 正規表現パターンの定義
- [x] エラーハンドリング
- [x] テスト用サンプルデータ作成: `tests/fixtures/sample_talk.txt`

**テスト計画**:
- 単体テスト: `tests/unit/test_parser.py`
  - [x] 正常系: 標準的なトーク履歴の解析
  - [x] 異常系: 不正なフォーマット
  - [x] 境界値: 空ファイル、1メッセージのみ
  - [x] 画像・スタンプ行の除外確認
  - [x] 複数日付にまたがるデータ
  - [x] 特殊文字を含むユーザー名・メッセージ

**依存**: PR#2（コード品質ツール）

---

#### PR#5: 形態素解析サービスの実装
**目的**: MeCabによる単語抽出

**タスク**:
- [x] `app/services/morphological.py`の実装
  - `MorphologicalAnalyzer`クラス
    - `analyze()`メソッド: テキストを単語に分解
    - `_filter_by_pos()`メソッド: 品詞フィルタリング
    - `_filter_by_length()`メソッド: 文字数フィルタリング
- [x] MeCab初期化処理
- [x] 品詞マッピングの定義
- [x] エラーハンドリング

**テスト計画**:
- 単体テスト: `tests/unit/test_morphological.py`
  - 正常系: 一般的な日本語文の解析
  - 各品詞の抽出確認
  - フィルタリング機能の確認
  - 空文字列の処理
  - 記号のみの文字列
  - 英数字混在テキスト

**依存**: PR#1（Docker環境でMeCabが必要）、PR#2（コード品質ツール）

---

### Phase 3: ビジネスロジック層の実装（PR#4, PR#5完了後）

#### PR#6: 単語カウンターの実装
**目的**: 単語とメッセージの集計

**タスク**:
- [x] `app/services/word_counter.py`の実装
  - `WordCounter`クラス
    - `count_morphological_words()`メソッド: 形態素解析結果の集計
    - `count_full_messages()`メソッド: メッセージ全文の集計
    - `_find_partial_matches()`メソッド: 部分一致検索
- [x] カウント結果のデータ構造定義
- [x] 出現情報の記録処理

**テスト計画**:
- 単体テスト: `tests/unit/test_word_counter.py`
  - [x] 形態素解析結果のカウント
  - [x] メッセージ全文の完全一致カウント
  - [x] メッセージ全文の部分一致カウント
  - [x] 同一単語が複数のメッセージに出現するケース
  - [x] 1メッセージ内に対象文字列が複数回出現するケース（非重複カウント）
  - [x] 空のデータセット
  - [x] 大量データでのパフォーマンス（オプション）

**依存**: PR#4（パーサー）、PR#5（形態素解析）

**完了**: ✅

---

#### PR#7: 統合解析サービスの実装
**目的**: 全処理を統合

**タスク**:
- [x] `app/services/analyzer.py`の実装
  - `TalkAnalyzer`クラス
    - `analyze()`メソッド: 統合解析処理
    - `_format_response()`メソッド: レスポンス整形
    - `_filter_by_period()`メソッド: 期間フィルタリング
- [x] 各サービスの連携処理
- [x] ソート・上位N件抽出
- [x] エラーハンドリング
- [x] 期間指定機能（start_date/end_date）の実装

**テスト計画**:
- 単体テスト: `tests/unit/test_analyzer.py`
  - [x] エンドツーエンドの解析処理
  - [x] 上位N件の取得確認
  - [x] 期間の計算確認
  - [x] 統計情報の正確性
  - [x] 期間指定機能のテスト（6件）
- 統合テスト（次Phaseに含む）

**依存**: PR#6（単語カウンター）

**完了**: ✅

---

### Phase 4: API層の実装（PR#7完了後）

#### PR#8: FastAPIアプリケーション構築
**目的**: REST APIの提供

**タスク**:
- [x] `app/main.py`の実装
  - FastAPIアプリケーションの初期化
  - CORSミドルウェアの設定
  - ルーターの登録
- [x] `app/core/config.py`の実装
  - 環境変数読み込み
  - 設定クラス定義
- [x] `app/core/cors.py`の実装
  - CORS設定
- [x] `app/api/v1/router.py`の実装
  - APIルーターの統合
- [x] `app/api/v1/endpoints/health.py`の実装
  - ヘルスチェックエンドポイント
- [x] `app/api/v1/endpoints/analyze.py`の実装
  - 解析エンドポイント
  - ファイルアップロード処理
  - バリデーション
  - エラーハンドリング

**テスト計画**:
- 統合テスト: `tests/integration/test_api.py`
  - [x] ヘルスチェックエンドポイント
  - [x] 解析エンドポイントの正常系
  - [x] 解析エンドポイントの異常系
    - [x] ファイルなし
    - [x] 不正なファイル形式
    - [x] ファイルサイズ超過
  - [x] CORS設定の確認
  - [x] 各種パラメータの動作確認

**依存**: PR#7（統合解析サービス）

**完了**: ✅

---

### Phase 5: 総合テストとドキュメント（全PR完了後）

#### PR#9: E2Eテストと最終調整
**目的**: 本番環境への準備

**タスク**:
- [x] E2Eテストの実装
  - 実際のLINEトーク履歴を使用した解析
    - talk/sample.txtの2025年分を解析
    - 結果を簡易的に表示、確認
  - レスポンス時間の測定
    - sample.txtは約18MB。解析に10秒以内を目標
    - メモリ使用量の監視
- [x] パフォーマンス最適化
    - 部分一致検索のO(N²)問題を発見（387秒 → 2.5秒に154倍高速化）
    - 部分一致検索を完全削除してコードをシンプル化
- [x] 流行語品質の改善
    - 最小単語長を2文字に変更（1文字ノイズ除外）
    - ストップワード機能実装（67単語除外）
    - 基本形表示に統一（活用形 → 辞書形）
- [x] README.mdの完成
  - API仕様の詳細
  - 使用例（curl、Python、JavaScript）
  - ストップワード機能の説明
  - パフォーマンス情報
  - トラブルシューティング

**テスト計画**:
- [x] 全ての単体テストをパス（125テスト）
- [x] 全ての統合テストをパス（16テスト）
- [x] E2Eテスト（2テスト）
  - 2025年分: 2.48秒、41,539メッセージ ✅
  - 全期間: 9.82秒、272,878メッセージ ✅
- [x] 実際のトーク履歴で動作確認

**依存**: PR#1〜PR#8すべて

**完了**: ✅

---

### Phase 6: デプロイと公開（PR#9完了後）

#### PR#10: Renderへのデプロイ
**目的**: バックエンドAPIを本番環境に公開

**タスク**:
- [ ] Renderアカウントの作成
  - https://render.com でGitHubアカウントを使用してサインアップ
- [ ] Web Serviceの作成
  - Dashboard → "New" → "Web Service"
  - GitHubリポジトリ `line_talk_analyzer_backend` を接続
  - Root Directoryを `.` に設定
- [ ] デプロイ設定
  ```yaml
  Name: line-talk-analyzer-api
  Environment: Docker
  Region: Singapore (最寄りのアジアリージョン)
  Branch: main
  Dockerfile Path: ./Dockerfile
  Docker Build Context Directory: ./
  Plan: Free
  ```
- [ ] 環境変数の設定
  ```
  PORT=8001
  ALLOWED_ORIGINS=http://localhost:3000,https://<vercel-url>.vercel.app
  ```
  ※ Vercelデプロイ後にURLを追加更新
- [ ] デプロイの実行
  - "Create Web Service" をクリック
  - 自動的にDockerビルド＆デプロイが開始
  - URLが発行される（例: `https://line-talk-analyzer-api.onrender.com`）
- [ ] 動作確認
  - ヘルスチェック: `https://<your-url>.onrender.com/api/v1/health`
  - レスポンスが返ることを確認
- [ ] 注意事項の文書化
  - 無料プランは15分アイドルでスリープ
  - 初回アクセス時は起動に30秒程度かかる
  - 月750時間制限（実質24/7稼働可能）

**テスト計画**:
- [ ] デプロイ成功の確認
- [ ] ヘルスチェックエンドポイントの動作確認
- [ ] 解析エンドポイントの動作確認（curl/Postmanで実行）
- [ ] CORS設定の動作確認
- [ ] スリープからの復帰テスト（15分以上放置後にアクセス）

**依存**: PR#9（E2Eテストと最終調整）

**完了**: 

---

#### Issue#01: appearancesフィールドの削除
**目的**: レスポンスデータサイズの削減によるモバイル対応の改善

**背景**:
- デプロイ後のスマホでの動作確認で問題が発覚
- 会話量の多いトーク履歴（約27万メッセージ）の解析結果が約4MBと巨大
- フロントエンド側でセッションストレージに保存しようとすると容量制限でエラー
- 試験的に`appearances`フィールドを削除したところ、0.05MBまで削減（約80分の1）
- `appearances`はもともと時系列解析などの将来的な拡張を見越して用意したもの
- しかし、現時点でフロントエンドでは使用されておらず、データ量だけが増大している
- **教訓**: フロントエンドに渡すデータは、バックエンド側で適切に処理・集約してから最小限のデータのみを返すべき

**タスク**:
- [x] `app/models/response.py`の修正
  - `WordAnalysisResult`から`appearances`フィールドを削除
  - `MessageAnalysisResult`から`appearances`フィールドを削除
  - `UserWordAnalysisResult`から`appearances`フィールドを削除（存在する場合）
  - `UserMessageAnalysisResult`から`appearances`フィールドを削除（存在する場合）
- [x] `app/services/word_counter.py`の修正
  - `appearances`収集処理をコメントアウト
  - 将来の時系列解析のために処理ロジックは保持
  - コメントで削除理由と今後の拡張方法を明記
- [x] `app/services/analyzer.py`の修正
  - `appearances`に関する処理をコメントアウト
  - レスポンス整形時に`appearances`を含めない
  - 将来の拡張のためのコメントを追加
- [x] テストコードの修正
  - `tests/unit/test_models.py`: `appearances`の検証を削除
  - `tests/unit/test_word_counter.py`: `appearances`のテストをコメントアウト（処理は残すため）
  - `tests/unit/test_analyzer.py`: `appearances`のアサーションを削除
  - `tests/integration/test_api.py`: APIレスポンスの`appearances`チェックを削除
  - `tests/e2e/test_real_data.py`: `appearances`の検証を削除
- [x] ドキュメントの更新
  - `README.md`: レスポンス例から`appearances`を削除
  - `doc/SPEC.md`: 
    - セクション3.2.1のレスポンス例を更新
    - Issue#01の完了チェック

**将来の拡張に向けた方針**:
- 時系列解析、ユーザー行動分析などの機能を追加する際は、以下のアプローチを検討：
  1. **専用エンドポイントの追加**: `/api/v1/analyze/timeline`など、詳細データが必要な場合のみ呼び出す
  2. **ページネーション**: `appearances`を返す場合は、limit/offsetで分割取得
  3. **データベース保存**: 解析結果をDB保存し、必要に応じてクエリで取得
  4. **サマリーデータのみ返却**: 日別集計、月別集計など、集約済みデータを返す
- コメントアウトした処理は、上記実装時の参考コードとして活用

**影響範囲**:
- レスポンスモデル: 3ファイル
- サービス層: 2ファイル
- テストコード: 5ファイル
- ドキュメント: 2ファイル

**期待される効果**:
- レスポンスサイズ: 約4MB → 約0.05MB（約80倍削減）
- モバイルブラウザでのセッションストレージ保存が可能に
- ネットワーク転送速度の向上
- フロントエンドのメモリ使用量削減
- ユーザー体験の改善

**テスト計画**:
- [x] 全ての単体テストがパスすること
- [x] 全ての統合テストがパスすること
- [x] E2Eテストで実際のレスポンスサイズを確認
  - 2025年分（41,539メッセージ）: 50KB以下を目標
  - 全期間（272,878メッセージ）: 200KB以下を目標
- [x] デプロイ後、スマホでの動作確認
  - セッションストレージへの保存が成功すること
  - 解析結果の表示が正しく動作すること

**依存**: PR#9（E2Eテストと最終調整）

**完了**: ✅

---

#### Issue#02: 改行されたメッセージを正しくカウントする

**目的**: LINEトーク履歴における改行を含むメッセージを正しく解析・集計する

**背景**:
- LINEのトーク履歴エクスポート機能では、改行を含むメッセージはダブルクォーテーション（`"`）で囲われる仕様となっている
- 現在のパーサーは1行単位でメッセージを解析するため、以下の問題が発生：
  1. 改行を含むメッセージの1行目のみを解析してしまう
  2. メッセージ本文に`"`が含まれた状態でカウントされる
  3. 2行目以降が無視され、単語解析の対象外となる
  4. 不完全なメッセージとして集計され、流行語の精度が低下する

**問題の具体例**:

元のメッセージ:
```
今日は天気が良かったです
明日も晴れるといいな
```

トーク履歴ファイルでの記載:
```
23:01	山田太郎	"今日は天気が良かったです
明日も晴れるといいな"
```

現在の動作（問題あり）:
- 解析されるメッセージ: `"今日は天気が良かったです`
- 2行目の`明日も晴れるといいな"`は無視される
- 単語解析も1行目のみが対象となる

期待される動作:
- 解析されるメッセージ: `今日は天気が良かったです\n明日も晴れるといいな`
- 改行を含む全文が1つのメッセージとして扱われる
- 単語解析も全行が対象となる

**仕様**:

1. **改行メッセージの検出条件**
   - メッセージの1文字目が`"`である
   - かつ、メッセージの末尾が`"`ではない（1行で完結していない）
   - かつ、次の行がメッセージ行の形式（`HH:MM\tユーザー名\t`）ではない

2. **複数行メッセージの結合処理**
   - 改行メッセージと判定された場合、次の行以降を順次読み込む
   - 末尾が`"`の行が出現するまで、または次のメッセージ行が出現するまで結合
   - 結合時は改行文字（`\n`）を保持する
   - 先頭と末尾の`"`を除去して、本来のメッセージ内容のみを抽出

3. **エッジケース**
   - メッセージ内に`"`が含まれる場合（エスケープなし）: 検証必要
   - 改行のみのメッセージ: 空メッセージとして除外
   - ファイル末尾で閉じ`"`がない場合: エラーハンドリング
   - 連続する複数の改行: 改行文字として保持

4. **単語解析の対応**
   - 形態素解析は改行を含む全文に対して実行
   - 改行文字は単語の区切りとして扱う（MeCabが自動的に処理）
   - メッセージ全文カウントも改行を含む全文を対象とする

**修正対象ファイル**:

1. **`app/services/parser.py`** ⭐ 主要修正
   - `_parse_message_line()`メソッドの拡張
     - 改行メッセージの検出ロジック追加
     - 複数行読み込み処理の実装
   - `parse()`メソッドの修正
     - 行単位ループから、状態管理を含むループへ変更
     - イテレータパターンの導入または行バッファリング
   - 新規メソッドの追加
     - `_is_multiline_message_start()`メソッド: 改行メッセージ開始判定
     - `_is_message_line()`メソッド: 次の行がメッセージ行かどうかの判定
     - `_read_multiline_message()`メソッド: 複数行の読み込みと結合

2. **`tests/fixtures/sample_talk.txt`**
   - 改行メッセージのテストケース追加
     - 標準的な2行メッセージ
     - 3行以上のメッセージ
     - 改行のみのメッセージ
     - 末尾に`"`がないケース（エラーケース）

3. **`tests/unit/test_parser.py`**
   - 改行メッセージのテストケース追加（7件）
     - 正常系: 2行メッセージの解析
     - 正常系: 3行以上のメッセージの解析
     - 正常系: 連続する改行の処理
     - 正常系: メッセージ内容に`"`が含まれるケース
     - 異常系: 閉じ`"`がない場合
     - 境界値: 改行のみのメッセージ
     - 統合: 通常メッセージと改行メッセージが混在

4. **`tests/integration/test_api.py`**（オプション）
   - 改行メッセージを含むトーク履歴での解析確認
   - レスポンスに改行メッセージ由来の単語が含まれることの確認

5. **`tests/e2e/test_real_data.py`**
   - 実際のsample.txtでの改行メッセージ処理確認
   - 改行メッセージの件数レポート追加

**実装方針**:

##### Phase 1: パーサーのリファクタリング
- `parse()`メソッドを行リストベースに変更（インデックスアクセス可能に）
- または、イテレータをpeekableにラップ

##### Phase 2: 改行メッセージ検出機能の実装
- `_is_multiline_message_start()`の実装
- 正規表現による判定ロジック

##### Phase 3: 複数行読み込み機能の実装
- `_read_multiline_message()`の実装
- 行結合とクォート除去

##### Phase 4: テストの実装
- 単体テスト、統合テスト、E2Eテストの順に実装

##### Phase 5: 実データでの検証
- sample.txtでの動作確認
- パフォーマンス影響の確認（目標: 10秒以内維持）

**期待される効果**:
- 改行を含むメッセージが正しくカウントされる
- 流行語の精度向上（2行目以降の単語も集計される）
- メッセージ全文カウントの精度向上
- ユーザーが実際に送信した内容に忠実な解析結果

**パフォーマンス考慮**:
- ファイル全体を事前にリストに読み込む場合、メモリ使用量が増加
  - 50MBファイル（約27万メッセージ）でも影響は軽微（数十MB増）
  - 現在のMeCab処理のほうがボトルネック（2.5秒程度）
- 改行メッセージの出現頻度は低い（全メッセージの1%未満と推定）
- 追加処理による影響は0.1秒未満と予想

**テスト計画**:
- [x] 単体テスト: `tests/unit/test_parser.py`
  - [x] 2行の改行メッセージの解析
  - [x] 3行以上の改行メッセージの解析
  - [x] 連続する改行（`\n\n`）の処理
  - [x] メッセージ内に`"`が含まれるケース
  - [x] 閉じ`"`がない異常ケース
  - [x] 改行のみのメッセージ（空メッセージ扱い）
  - [x] 通常メッセージと改行メッセージの混在
- [x] 統合テスト: `tests/integration/test_api.py`
  - [x] 改行メッセージを含むファイルの解析
  - [x] 改行メッセージ由来の単語が結果に含まれることの確認
- [x] E2Eテスト: `tests/e2e/test_real_data.py`
  - [x] sample.txtでの改行メッセージ処理確認
  - [x] パフォーマンス影響の測定（10秒以内を維持）
  - [x] 改行メッセージ件数のレポート

**実測値**:
- 改行メッセージ数：6,339件（全272,878メッセージ中の2.32%）
- 最大改行数：8行
- パフォーマンス影響：なし（想定通り）

**依存**: なし（独立したIssueとして実装可能）

**優先度**: 高（流行語精度に直接影響）

**完了**: ✅

---

#### Issue#03: 名詞が連続する場合1つの名詞とみなす

**目的**: 形態素解析で分割された固有名詞などの複合語を正しく1単語として集計する

**背景**:
- MeCabの形態素解析では、固有名詞や複合語が複数の名詞に分割されることがある
- 例: 「ガンダム」→「ガン」「ダム」、「機動戦士」→「機動」「戦士」、「プレイステーション」→「プレイ」「ステーション」
- これにより、本来1つの単語として認識されるべき固有名詞が別々にカウントされ、流行語ランキングの精度が低下
- ユーザーが実際に使用している固有名詞や複合語を正しく抽出できない

**問題の具体例**:

元のメッセージ:
```
ガンダム見た
機動戦士ガンダムが好き
```

現在の動作（問題あり）:
- 抽出される単語: 「ガン」「ダム」「見る」「機動」「戦士」「ガン」「ダム」「好き」
- カウント結果: 「ガン」2回、「ダム」2回、「機動」1回、「戦士」1回
- 「ガンダム」という固有名詞が認識されない

期待される動作:
- 抽出される単語: 「ガンダム」「見る」「機動戦士ガンダム」「好き」
- カウント結果: 「ガンダム」2回、「機動戦士ガンダム」1回
- 固有名詞として正しく認識される

**仕様**:

1. **名詞連続の定義**
   - MeCabの形態素解析結果で、名詞の品詞が連続して出現する場合
   - 対象となる名詞の品詞（品詞の大分類が「名詞」）:
     - 名詞-一般: 普通の名詞（例: 「プラモデル」「アニメ」「ゲーム」）
     - 名詞-固有名詞: 人名、地名、作品名など（例: 「ガンダム」「東京」「太郎」）
   - 除外する名詞:
     - 名詞-サ変接続: 「する」をつけて動詞になる名詞（例: 「勉強」「運動」「仕事」）
     - 名詞-形容動詞語幹: 「だ」をつけて形容動詞になる名詞（例: 「綺麗」「元気」「便利」）
     - 名詞-代名詞: 「これ」「それ」「あれ」など
     - 名詞-非自立: 「もの」「こと」など、単独では使われにくい名詞
     - 名詞-数: 数詞（例: 「1」「2」「3」「十」「百」）※不要な結合を防ぐため
     - 名詞-接尾: 接尾辞（例: 「さん」「円」「個」「回」）※単独では意味を持たないため

2. **連続名詞の結合処理**
   - 形態素解析結果を1つずつ走査
   - 名詞が出現したら、次の形態素も名詞かどうかを確認
   - 名詞が連続する限り、それらを結合して1つの単語とする
   - 結合時は元の表層形（表記）をそのまま連結（スペースなし）
   - 結合された単語の品詞は「名詞」として扱う
   - 結合された単語は基本形も表層形と同じにする

3. **エッジケース**
   - 2つの名詞の結合: 「機動」+「戦士」→「機動戦士」
   - 3つ以上の名詞の結合: 「機動」+「戦士」+「ガンダム」→「機動戦士ガンダム」
   - 名詞と他の品詞が混在: 「見た ガンダム を」→「見た」「ガンダム」「を」（名詞のみ結合）
   - 1文字名詞の連続: 「お+茶」→「お茶」（結合する）
   - 助詞・助動詞を挟む: 「ガンダム の プラモデル」→「ガンダム」「の」「プラモデル」（結合しない）

4. **既存機能への影響**
   - ストップワード除外: 連結後の単語がストップワードに該当する場合は除外
   - 最小単語長フィルタ: 連結後の単語長で判定
   - 基本形表示: 連結された単語はそのまま表示（基本形への変換は行わない）

**具体的な処理例**:

入力文: 「機動戦士ガンダムのプラモデルを作った」

MeCab解析結果:
```
機動    名詞,一般
戦士    名詞,一般
ガン    名詞,一般
ダム    名詞,一般
の      助詞,連体化
プラモデル  名詞,一般
を      助詞,格助詞
作っ    動詞,自立
た      助動詞
```

結合処理後:
```
機動戦士ガンダム  名詞  (「機動」「戦士」「ガン」「ダム」を結合)
の      助詞,連体化
プラモデル  名詞,一般
を      助詞,格助詞
作る    動詞,自立  (基本形)
た      助動詞
```

最終的な単語リスト（品詞フィルタ後）:
- 「機動戦士ガンダム」（名詞）
- 「プラモデル」（名詞）
- 「作る」（動詞）

**修正対象ファイル**:

1. **`app/services/morphological.py`** ⭐ 主要修正
   - `analyze()`メソッドの修正
     - 形態素解析結果を走査して連続名詞を検出
     - 連続名詞の結合処理
   - 新規メソッドの追加
     - `_combine_consecutive_nouns()`メソッド: 連続名詞の結合処理
     - `_is_combinable_noun()`メソッド: 結合可能な名詞かどうかの判定
   - データ構造の調整
     - 結合された単語の情報（表層形、品詞、基本形）を適切に保持

2. **`tests/fixtures/sample_talk.txt`**
   - 連続名詞のテストケース追加
     - 2つの名詞が連続するケース
     - 3つ以上の名詞が連続するケース
     - 名詞と他の品詞が混在するケース

3. **`tests/unit/test_morphological.py`**
   - 連続名詞結合のテストケース追加（6件以上）
     - 正常系: 2つの名詞の結合（「機動」+「戦士」→「機動戦士」）
     - 正常系: 3つ以上の名詞の結合（「機動」+「戦士」+「ガンダム」→「機動戦士ガンダム」）
     - 正常系: 複数の連続名詞グループ（「機動戦士」と「プラモデル」が独立）
     - 境界値: 1文字名詞の連続（「お」+「茶」→「お茶」）
     - 境界値: 助詞で区切られる名詞（結合されない）
     - 統合: 連続名詞がストップワードに該当する場合の除外
     - 統合: 連続名詞が最小単語長未満の場合の除外

4. **`tests/unit/test_word_counter.py`**
   - 連続名詞がカウントされることの確認
   - 分割された名詞が個別にカウントされないことの確認

5. **`tests/integration/test_api.py`**（オプション）
   - 連続名詞を含むトーク履歴での解析確認
   - レスポンスに連続名詞が正しく含まれることの確認

6. **`tests/e2e/test_real_data.py`**
   - 実際のsample.txtでの連続名詞処理確認
   - 連続名詞の結合件数レポート追加
   - 流行語ランキングの変化確認

**実装方針**:

##### Phase 1: 連続名詞判定機能の実装
- `_is_combinable_noun()`メソッドの実装
- 結合可能な名詞の品詞パターン定義
- 除外する名詞の品詞パターン定義

##### Phase 2: 連続名詞結合機能の実装
- `_combine_consecutive_nouns()`メソッドの実装
- 形態素解析結果を走査して連続名詞を検出
- 表層形を連結して新しい単語を生成
- 品詞情報の調整

##### Phase 3: analyze()メソッドへの統合
- 形態素解析後に連続名詞結合処理を実行
- 既存のフィルタリング処理との整合性確保

##### Phase 4: テストの実装
- 単体テスト、統合テスト、E2Eテストの順に実装
- 既存のテストが全てパスすることを確認

##### Phase 5: 実データでの検証
- sample.txtでの動作確認
- 流行語ランキングの改善を確認
- パフォーマンス影響の確認（目標: 10秒以内維持）

**期待される効果**:
- 固有名詞や複合語が正しく1単語として集計される
- 流行語ランキングの精度向上（意味のある単語が上位に）
- ユーザーが実際に使用している言葉を正しく抽出
- 「ガン」「ダム」のような意味のない断片が減少

**パフォーマンス考慮**:
- 連続名詞の結合処理は形態素解析結果の1回の走査で完了
- O(N)の計算量（Nは形態素数）
- 実際の影響は軽微（形態素解析自体の処理時間が支配的）
- 追加処理による影響は0.1秒未満と予想

**注意事項**:
- MeCabの辞書によっては既に固有名詞が正しく認識される場合もある
  - mecab-ipadic-neologdは新語に対応しているが、全ての固有名詞を網羅しているわけではない
- 過度な結合を避けるため、助詞・助動詞を挟む場合は結合しない
- 「これもの」のような不自然な結合を避けるため、代名詞・非自立名詞は除外

**テスト計画**:
- [x] 単体テスト: `tests/unit/test_morphological.py`
  - [x] 2つの名詞の結合
  - [x] 3つ以上の名詞の結合
  - [x] 複数の連続名詞グループ
  - [x] 1文字名詞の連続
  - [x] 助詞で区切られる名詞
  - [x] 連続名詞とストップワードの組み合わせ
  - [x] 連続名詞と最小単語長フィルタの組み合わせ
- [x] 単体テスト: `tests/unit/test_word_counter.py`
  - [x] 連続名詞のカウント確認
  - [x] 分割された名詞が個別にカウントされないことの確認
- [x] 統合テスト: `tests/integration/test_api.py`
  - [x] 連続名詞を含むファイルの解析
  - [x] レスポンスに連続名詞が含まれることの確認
- [x] E2Eテスト: `tests/e2e/test_real_data.py`
  - [x] sample.txtでの連続名詞処理確認
  - [x] パフォーマンス影響の測定（10秒以内を維持）
  - [x] 連続名詞の結合件数レポート
  - [x] 流行語ランキングTOP10の変化確認

**実装内容（追加）**:
- [x] URL除外機能の実装
  - [x] `URL_PATTERN`正規表現の定義（`r"https?://\S+"`）
  - [x] `_remove_urls()`メソッドの実装（改行保持）
  - [x] パーサーへの統合
  - [x] テストケースの追加（URL除外3件）
- [x] MeCab辞書の明示化
  - [x] `MorphologicalAnalyzer.__init__()`でneologd辞書を明示的に指定
  - [x] 変更前: `MeCab.Tagger()`（デフォルト辞書）
  - [x] 変更後: `MeCab.Tagger("-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd")`
  - [x] 記号の正しい認識: 「！#」→ 記号-一般（以前は名詞-サ変接続と誤認識）
  - [x] DEFAULT_TARGET_POSフィルタで記号を自動除外
  - [x] テストケース3件をneologd辞書の動作に合わせて修正
- [x] 絵文字のテキスト変換防止機能の実装
  - [x] 問題: neologd辞書が絵文字を日本語テキストに変換（例: 😭 → 「泣き顔」「大泣き」）
  - [x] `_contains_emoji()`関数の実装（Unicode範囲による絵文字判定）
  - [x] 絵文字の場合は基本形ではなく表層形（絵文字そのまま）を使用
  - [x] 絵文字を含む記号を品詞フィルタで特別に許可
  - [x] テストケースの追加（絵文字処理5件）
  - [x] 効果: 😭が「泣き顔」ではなく「😭」として正しく集計される

**実測値**:
- 解析時間: 3.48秒（2025年分41,142メッセージ）
- 全146テスト成功
- URL関連単語: 0件（完全除外）
- 記号単語: 0件（正しく除外）
- 連続名詞結合成功例: 「dアニメストア」「機動戦士ガンダム」「プレイステーション」
- 絵文字の正しい集計: 😭が「泣き顔」ではなく「😭」として1024回カウント（実際の使用頻度を反映）

**依存**: なし（独立したIssueとして実装可能）

**優先度**: 高（流行語精度に直接影響）

**完了**: ✅ 

---

## 9. テスト戦略

### 9.1 テストレベル

| レベル | ツール | カバレッジ目標 |
|--------|--------|---------------|
| 単体テスト | pytest | 80%以上 |
| 統合テスト | pytest + TestClient | 主要フロー100% |
| E2Eテスト | 実データ | 実用性確認 |

### 9.2 テストデータ

- **最小データセット**: 1日分、2ユーザー、10メッセージ
- **標準データセット**: 1ヶ月分、3ユーザー、500メッセージ
- **大規模データセット**: 1年分、5ユーザー、10000メッセージ
- **異常データセット**: 不正フォーマット、特殊文字、空ファイル

### 9.3 CI/CD

- PRごとに自動テスト実行
- コードカバレッジレポート生成
- リントとフォーマットチェック
- 型チェック（mypy）

---

## 10. 非機能要件

### 10.1 パフォーマンス

- 1万メッセージの解析を10秒以内で完了
- 最大ファイルサイズ: 50MB
- 同時リクエスト数: 10（初期）

### 10.2 セキュリティ

- ファイルアップロードのバリデーション
- ファイルサイズ制限
- タイムアウト設定（30秒）
- 悪意のあるファイル内容の検証（オプション）

### 10.3 可用性

- ヘルスチェックエンドポイントの提供
- エラーログの出力
- グレースフルシャットダウン

---

## 11. 今後の拡張案

- データベースへの解析結果保存
- ユーザー認証機能
- 複数ファイルの一括解析
- 時系列グラフ用のデータ出力
- 感情分析の追加
- ユーザー別統計情報
- ワードクラウド用データ生成

---

## 12. 参考資料

- [FastAPI公式ドキュメント](https://fastapi.tiangolo.com/)
- [MeCab公式サイト](https://taku910.github.io/mecab/)
- [mecab-ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd)
- [PEP8スタイルガイド](https://pep8-ja.readthedocs.io/ja/latest/)
