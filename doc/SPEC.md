# LINEトーク履歴解析バックエンド 仕様書

## 1. 概要

本プロジェクトは、LINEのトーク履歴を解析し、1年間の流行語大賞を表示するスマホ用Webアプリケーションのバックエンドです。

### 1.1 主な機能

- LINEトーク履歴ファイル（.txt形式）のアップロードと解析
- 形態素解析による単語の抽出と集計
- メッセージ全文を単語としてカウントする機能
- 集計結果をJSON形式で返すRESTful API
- フロントエンドとのCORS対応

### 1.2 技術スタック

- **言語**: Python 3.11+
- **Webフレームワーク**: FastAPI
- **形態素解析**: MeCab + mecab-python3
- **辞書**: mecab-ipadic-neologd（新語対応）
- **テストフレームワーク**: pytest
- **コンテナ**: Docker + Docker Compose
- **コード品質**: Black, isort, flake8, mypy

---

## 2. システムアーキテクチャ

```
[フロントエンド] ←→ [FastAPI] ←→ [LINEトーク解析エンジン]
                                        ↓
                                   [形態素解析(MeCab)]
```

### 2.1 ディレクトリ構成

```
line_talk_analyzer_backend/
├── app/
│   ├── __init__.py
│   ├── main.py                    # FastAPIアプリケーションのエントリポイント
│   ├── api/
│   │   ├── __init__.py
│   │   ├── v1/
│   │   │   ├── __init__.py
│   │   │   ├── endpoints/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── analyze.py    # 解析エンドポイント
│   │   │   │   └── health.py     # ヘルスチェック
│   │   │   └── router.py
│   ├── core/
│   │   ├── __init__.py
│   │   ├── config.py              # 設定管理
│   │   └── cors.py                # CORS設定
│   ├── models/
│   │   ├── __init__.py
│   │   ├── request.py             # リクエストモデル
│   │   └── response.py            # レスポンスモデル
│   ├── services/
│   │   ├── __init__.py
│   │   ├── parser.py              # LINEトーク履歴パーサー
│   │   ├── morphological.py      # 形態素解析
│   │   ├── word_counter.py       # 単語カウンター
│   │   └── analyzer.py            # 統合解析サービス
│   └── utils/
│       ├── __init__.py
│       └── validators.py          # バリデーション関数
├── tests/
│   ├── __init__.py
│   ├── conftest.py                # pytest設定
│   ├── fixtures/
│   │   └── sample_talk.txt        # テスト用サンプルデータ
│   ├── unit/
│   │   ├── __init__.py
│   │   ├── test_parser.py
│   │   ├── test_morphological.py
│   │   ├── test_word_counter.py
│   │   └── test_analyzer.py
│   └── integration/
│       ├── __init__.py
│       └── test_api.py
├── docker-compose.yml
├── Dockerfile
├── requirements.txt
├── requirements-ci.txt
├── mypy.ini
├── .dockerignore
├── .gitignore
└── README.md
```

---

## 3. データ形式

### 3.1 入力データ形式（LINEトーク履歴）

```
[LINE] サンプルグループのトーク履歴
保存日時：2024/08/01 00:00

2024/08/01(木)
22:12	hoge山fuga太郎	おはようございます
22:13		piyo田が参加しました。
22:14	piyo田	こんにちは
22:15	hoge山fuga太郎	[スタンプ]
22:16	foo子	よろしくお願いします
```

**フォーマット**:
- ヘッダー行（1行目）: `[LINE] <トーク名>のトーク履歴`
- 保存日時行（2行目）: `保存日時：YYYY/MM/DD HH:MM`
- 空行（3行目）
- 日付行: `YYYY/MM/DD(曜日)`
- メッセージ行: `HH:MM<TAB>ユーザー名<TAB>メッセージ本文`
  - 区切り文字はタブ文字
  - ユーザー名が空の場合もあり（システムメッセージ）
- 画像/スタンプ等: `HH:MM<TAB>ユーザー名<TAB>[スタンプ]` または `HH:MM<TAB>ユーザー名<TAB>[写真]`

### 3.2 APIレスポンス形式

#### 3.2.1 解析結果レスポンス

```json
{
  "status": "success",
  "data": {
    "analysis_period": {
      "start_date": "2024-01-01",
      "end_date": "2024-12-31"
    },
    "total_messages": 1500,
    "total_users": 3,
    "morphological_analysis": {
      "top_words": [
        {
          "word": "おうち",
          "count": 42,
          "part_of_speech": "名詞"
        }
      ]
    },
    "full_message_analysis": {
      "top_messages": [
        {
          "message": "おうち帰りたい",
          "count": 23
        }
      ]
    }
  }
}
```

#### 3.2.2 エラーレスポンス

```json
{
  "status": "error",
  "error": {
    "code": "INVALID_FILE_FORMAT",
    "message": "アップロードされたファイルの形式が無効です"
  }
}
```

---

## 4. API仕様

### 4.1 エンドポイント一覧

| メソッド | パス | 説明 |
|---------|------|------|
| GET | `/api/v1/health` | ヘルスチェック |
| POST | `/api/v1/analyze` | トーク履歴解析 |

### 4.2 詳細仕様

#### 4.2.1 ヘルスチェック

```
GET /api/v1/health
```

**レスポンス**:
```json
{
  "status": "ok",
  "version": "1.0.0"
}
```

#### 4.2.2 トーク履歴解析

```
POST /api/v1/analyze
```

**リクエスト** (multipart/form-data):
- `file`: LINEトーク履歴ファイル（.txt形式）
- `top_n`: 取得する上位単語数（オプション、デフォルト: 50）
- `min_word_length`: 最小単語長（オプション、デフォルト: 1）
- `exclude_parts`: 除外品詞（オプション、カンマ区切り）

**レスポンス**: 3.2.1参照

**エラーコード**:
- `400`: ファイルが指定されていない、ファイル形式が無効
- `413`: ファイルサイズが大きすぎる（上限: 50MB）
- `500`: サーバー内部エラー

---

## 5. 解析処理の詳細

### 5.1 LINEトーク履歴パーサー (`services/parser.py`)

**責務**:
- LINEトーク履歴ファイルを読み込み、構造化データに変換

**処理フロー**:
1. テキストファイルを読み込み、ヘッダーと保存日時行をスキップ
2. 行ごとに読み込み
3. 日付行（`YYYY/MM/DD(曜日)`形式）を検出して現在の日付を更新
4. メッセージ行をタブ文字で分割して解析（時刻、ユーザー名、メッセージ本文）
5. システムメッセージ（ユーザー名が空）、画像（`[写真]`）、スタンプ（`[スタンプ]`）等は除外
6. 構造化データ（リスト）として返却

**データ構造**:
```python
@dataclass
class Message:
    datetime: datetime
    user: str
    content: str
```

### 5.2 形態素解析サービス (`services/morphological.py`)

**責務**:
- MeCabを使用して単語に分解し、品詞情報を付与

**処理フロー**:
1. メッセージ本文をMeCabで形態素解析
2. 品詞情報を抽出
3. 除外品詞（助詞、助動詞など）をフィルタリング
4. 最小単語長でフィルタリング
5. 単語リストを返却

**抽出対象品詞**（デフォルト）:
- 名詞（一般、固有名詞、サ変接続など）
- 動詞（自立）
- 形容詞（自立）
- 副詞
- 感動詞

### 5.3 単語カウンター (`services/word_counter.py`)

**責務**:
- 形態素解析結果とメッセージ全文の集計

#### 5.3.1 形態素解析結果のカウント

1. 各単語の出現回数を集計
2. 単語ごとに出現したメッセージ情報を記録
3. 品詞情報を保持

#### 5.3.2 メッセージ全文のカウント

1. 各メッセージを1単語として扱う
2. 完全一致カウント: 同一メッセージの出現回数
3. 部分一致カウント: そのメッセージを部分文字列として含む他のメッセージ
   - 例: 「それな」を含む「それな；；」をカウント
4. 両方の出現情報を記録

### 5.4 統合解析サービス (`services/analyzer.py`)

**責務**:
- 上記サービスを統合し、API向けのレスポンスを生成

**処理フロー**:
1. パーサーでトーク履歴を構造化
2. 形態素解析で単語抽出
3. 単語カウンターで集計
4. 上位N件を抽出してソート
5. APIレスポンス形式に整形

---

## 6. CORS設定

フロントエンドからのアクセスを許可するため、以下の設定を行う：

- **許可オリジン**: 環境変数 `ALLOWED_ORIGINS` で設定（デフォルト: `["http://localhost:3000"]`）
- **許可メソッド**: `["GET", "POST"]`
- **許可ヘッダー**: `["*"]`
- **クレデンシャル**: `True`

---

## 7. 設定管理

環境変数を使用して設定を管理（`core/config.py`）:

| 変数名 | デフォルト値 | 説明 |
|--------|-------------|------|
| `APP_NAME` | "LINE Talk Analyzer" | アプリケーション名 |
| `APP_VERSION` | "1.0.0" | バージョン |
| `ALLOWED_ORIGINS` | "http://localhost:3000" | CORS許可オリジン（カンマ区切り） |
| `MAX_FILE_SIZE_MB` | 50 | 最大ファイルサイズ（MB） |
| `DEFAULT_TOP_N` | 50 | デフォルト上位取得数 |
| `MIN_WORD_LENGTH` | 1 | 最小単語長 |

---

## 8. 実装計画

**重要**: 各PRの作業が完了したら、以下を必ず実施すること：
1. `/app/doc/PR/PRxx.md`ファイルを作成（xxはPR番号）
   - 実装内容、テスト結果、修正内容などを詳細に記載
   - 既存のPR01.md、PR03.md、PR04.mdを参考にする
2. 本仕様書（SPEC.md）の該当PRのタスクチェックボックスにチェック（`[ ]` → `[x]`）を入れる

### Phase 1: プロジェクト基盤構築（並列実行可能）

#### PR#1: Docker環境セットアップ
**目的**: 開発環境のコンテナ化

**タスク**:
- [x] Dockerfileの作成
  - Python 3.11ベースイメージ
  - MeCabとmecab-ipadic-neologdのインストール
  - 依存パッケージのインストール
- [x] docker-compose.ymlの作成
  - APIサーバーコンテナ定義
  - ポートマッピング（8000:8000）
  - ボリュームマウント
- [x] .dockerignoreの作成
- [x] requirements.txtの更新
  - fastapi
  - uvicorn[standard]
  - mecab-python3
  - python-multipart
- [x] README.mdの更新（環境構築手順）

**テスト計画**:
- コンテナのビルドが成功すること
- コンテナ起動後にMeCabが使用可能なこと
- `docker-compose up`でサーバーが起動すること

**依存**: なし

---

#### PR#2: コード品質ツールのセットアップ
**目的**: コーディング規約の適用

**タスク**:
- [x] mypy.iniの作成（既存があれば確認）
- [x] .vscode/settings.jsonの確認・更新
- [x] .vscode/extensions.jsonの確認・更新
- [x] requirements-ci.txtの確認・更新
  - pytest
  - pytest-cov
  - pytest-asyncio
  - black
  - isort
  - flake8
  - mypy
- [x] .github/workflows/ci.ymlの確認・更新

**テスト計画**:
- 各ツールが正常に動作すること
- サンプルコードでリント・フォーマットが実行されること

**依存**: なし

---

### Phase 2: データ層の実装（並列実行可能）

#### PR#3: データモデル定義
**目的**: APIのリクエスト/レスポンスモデルの定義

**タスク**:
- [x] `app/models/request.py`の実装
  - `AnalyzeRequest`モデル
  - バリデーション定義
- [x] `app/models/response.py`の実装
  - `AnalysisResult`モデル
  - `WordAnalysisResult`モデル
  - `MessageAnalysisResult`モデル
  - `ErrorResponse`モデル
- [x] 型アノテーション完備
- [x] Docstring完備

**テスト計画**:
- 単体テスト: `tests/unit/test_models.py`
  - [x] 各モデルのインスタンス化
  - [x] バリデーションエラーのテスト
  - [x] JSONシリアライズ/デシリアライズのテスト

**依存**: PR#2（コード品質ツール）

---

#### PR#4: LINEトーク履歴パーサーの実装
**目的**: トーク履歴の構造化

**タスク**:
- [x] `app/services/parser.py`の実装
  - `Message`データクラス
  - `LineMessageParser`クラス
    - `parse()`メソッド: ファイルを読み込んで構造化
    - `_parse_date_line()`メソッド: 日付行の解析
    - `_parse_message_line()`メソッド: メッセージ行の解析
- [x] 正規表現パターンの定義
- [x] エラーハンドリング
- [x] テスト用サンプルデータ作成: `tests/fixtures/sample_talk.txt`

**テスト計画**:
- 単体テスト: `tests/unit/test_parser.py`
  - [x] 正常系: 標準的なトーク履歴の解析
  - [x] 異常系: 不正なフォーマット
  - [x] 境界値: 空ファイル、1メッセージのみ
  - [x] 画像・スタンプ行の除外確認
  - [x] 複数日付にまたがるデータ
  - [x] 特殊文字を含むユーザー名・メッセージ

**依存**: PR#2（コード品質ツール）

---

#### PR#5: 形態素解析サービスの実装
**目的**: MeCabによる単語抽出

**タスク**:
- [x] `app/services/morphological.py`の実装
  - `MorphologicalAnalyzer`クラス
    - `analyze()`メソッド: テキストを単語に分解
    - `_filter_by_pos()`メソッド: 品詞フィルタリング
    - `_filter_by_length()`メソッド: 文字数フィルタリング
- [x] MeCab初期化処理
- [x] 品詞マッピングの定義
- [x] エラーハンドリング

**テスト計画**:
- 単体テスト: `tests/unit/test_morphological.py`
  - 正常系: 一般的な日本語文の解析
  - 各品詞の抽出確認
  - フィルタリング機能の確認
  - 空文字列の処理
  - 記号のみの文字列
  - 英数字混在テキスト

**依存**: PR#1（Docker環境でMeCabが必要）、PR#2（コード品質ツール）

---

### Phase 3: ビジネスロジック層の実装（PR#4, PR#5完了後）

#### PR#6: 単語カウンターの実装
**目的**: 単語とメッセージの集計

**タスク**:
- [x] `app/services/word_counter.py`の実装
  - `WordCounter`クラス
    - `count_morphological_words()`メソッド: 形態素解析結果の集計
    - `count_full_messages()`メソッド: メッセージ全文の集計
    - `_find_partial_matches()`メソッド: 部分一致検索
- [x] カウント結果のデータ構造定義
- [x] 出現情報の記録処理

**テスト計画**:
- 単体テスト: `tests/unit/test_word_counter.py`
  - [x] 形態素解析結果のカウント
  - [x] メッセージ全文の完全一致カウント
  - [x] メッセージ全文の部分一致カウント
  - [x] 同一単語が複数のメッセージに出現するケース
  - [x] 1メッセージ内に対象文字列が複数回出現するケース（非重複カウント）
  - [x] 空のデータセット
  - [x] 大量データでのパフォーマンス（オプション）

**依存**: PR#4（パーサー）、PR#5（形態素解析）

**完了**: ✅

---

#### PR#7: 統合解析サービスの実装
**目的**: 全処理を統合

**タスク**:
- [x] `app/services/analyzer.py`の実装
  - `TalkAnalyzer`クラス
    - `analyze()`メソッド: 統合解析処理
    - `_format_response()`メソッド: レスポンス整形
    - `_filter_by_period()`メソッド: 期間フィルタリング
- [x] 各サービスの連携処理
- [x] ソート・上位N件抽出
- [x] エラーハンドリング
- [x] 期間指定機能（start_date/end_date）の実装

**テスト計画**:
- 単体テスト: `tests/unit/test_analyzer.py`
  - [x] エンドツーエンドの解析処理
  - [x] 上位N件の取得確認
  - [x] 期間の計算確認
  - [x] 統計情報の正確性
  - [x] 期間指定機能のテスト（6件）
- 統合テスト（次Phaseに含む）

**依存**: PR#6（単語カウンター）

**完了**: ✅

---

### Phase 4: API層の実装（PR#7完了後）

#### PR#8: FastAPIアプリケーション構築
**目的**: REST APIの提供

**タスク**:
- [x] `app/main.py`の実装
  - FastAPIアプリケーションの初期化
  - CORSミドルウェアの設定
  - ルーターの登録
- [x] `app/core/config.py`の実装
  - 環境変数読み込み
  - 設定クラス定義
- [x] `app/core/cors.py`の実装
  - CORS設定
- [x] `app/api/v1/router.py`の実装
  - APIルーターの統合
- [x] `app/api/v1/endpoints/health.py`の実装
  - ヘルスチェックエンドポイント
- [x] `app/api/v1/endpoints/analyze.py`の実装
  - 解析エンドポイント
  - ファイルアップロード処理
  - バリデーション
  - エラーハンドリング

**テスト計画**:
- 統合テスト: `tests/integration/test_api.py`
  - [x] ヘルスチェックエンドポイント
  - [x] 解析エンドポイントの正常系
  - [x] 解析エンドポイントの異常系
    - [x] ファイルなし
    - [x] 不正なファイル形式
    - [x] ファイルサイズ超過
  - [x] CORS設定の確認
  - [x] 各種パラメータの動作確認

**依存**: PR#7（統合解析サービス）

**完了**: ✅

---

### Phase 5: 総合テストとドキュメント（全PR完了後）

#### PR#9: E2Eテストと最終調整
**目的**: 本番環境への準備

**タスク**:
- [x] E2Eテストの実装
  - 実際のLINEトーク履歴を使用した解析
    - talk/sample.txtの2025年分を解析
    - 結果を簡易的に表示、確認
  - レスポンス時間の測定
    - sample.txtは約18MB。解析に10秒以内を目標
    - メモリ使用量の監視
- [x] パフォーマンス最適化
    - 部分一致検索のO(N²)問題を発見（387秒 → 2.5秒に154倍高速化）
    - 部分一致検索を完全削除してコードをシンプル化
- [x] 流行語品質の改善
    - 最小単語長を2文字に変更（1文字ノイズ除外）
    - ストップワード機能実装（67単語除外）
    - 基本形表示に統一（活用形 → 辞書形）
- [x] README.mdの完成
  - API仕様の詳細
  - 使用例（curl、Python、JavaScript）
  - ストップワード機能の説明
  - パフォーマンス情報
  - トラブルシューティング

**テスト計画**:
- [x] 全ての単体テストをパス（125テスト）
- [x] 全ての統合テストをパス（16テスト）
- [x] E2Eテスト（2テスト）
  - 2025年分: 2.48秒、41,539メッセージ ✅
  - 全期間: 9.82秒、272,878メッセージ ✅
- [x] 実際のトーク履歴で動作確認

**依存**: PR#1〜PR#8すべて

**完了**: ✅

---

### Phase 6: デプロイと公開（PR#9完了後）

#### PR#10: Renderへのデプロイ
**目的**: バックエンドAPIを本番環境に公開

**タスク**:
- [ ] Renderアカウントの作成
  - https://render.com でGitHubアカウントを使用してサインアップ
- [ ] Web Serviceの作成
  - Dashboard → "New" → "Web Service"
  - GitHubリポジトリ `line_talk_analyzer_backend` を接続
  - Root Directoryを `.` に設定
- [ ] デプロイ設定
  ```yaml
  Name: line-talk-analyzer-api
  Environment: Docker
  Region: Singapore (最寄りのアジアリージョン)
  Branch: main
  Dockerfile Path: ./Dockerfile
  Docker Build Context Directory: ./
  Plan: Free
  ```
- [ ] 環境変数の設定
  ```
  PORT=8001
  ALLOWED_ORIGINS=http://localhost:3000,https://<vercel-url>.vercel.app
  ```
  ※ Vercelデプロイ後にURLを追加更新
- [ ] デプロイの実行
  - "Create Web Service" をクリック
  - 自動的にDockerビルド＆デプロイが開始
  - URLが発行される（例: `https://line-talk-analyzer-api.onrender.com`）
- [ ] 動作確認
  - ヘルスチェック: `https://<your-url>.onrender.com/api/v1/health`
  - レスポンスが返ることを確認
- [ ] 注意事項の文書化
  - 無料プランは15分アイドルでスリープ
  - 初回アクセス時は起動に30秒程度かかる
  - 月750時間制限（実質24/7稼働可能）

**テスト計画**:
- [ ] デプロイ成功の確認
- [ ] ヘルスチェックエンドポイントの動作確認
- [ ] 解析エンドポイントの動作確認（curl/Postmanで実行）
- [ ] CORS設定の動作確認
- [ ] スリープからの復帰テスト（15分以上放置後にアクセス）

**依存**: PR#9（E2Eテストと最終調整）

**完了**: 

---

### Issue修正・改善タスク

#### Issue#01: appearancesフィールドの削除
**目的**: レスポンスデータサイズの削減によるモバイル対応の改善

**背景**:
- デプロイ後のスマホでの動作確認で問題が発覚
- 会話量の多いトーク履歴（約27万メッセージ）の解析結果が約4MBと巨大
- フロントエンド側でセッションストレージに保存しようとすると容量制限でエラー
- 試験的に`appearances`フィールドを削除したところ、0.05MBまで削減（約80分の1）
- `appearances`はもともと時系列解析などの将来的な拡張を見越して用意したもの
- しかし、現時点でフロントエンドでは使用されておらず、データ量だけが増大している
- **教訓**: フロントエンドに渡すデータは、バックエンド側で適切に処理・集約してから最小限のデータのみを返すべき

**タスク**:
- [x] `app/models/response.py`の修正
  - `WordAnalysisResult`から`appearances`フィールドを削除
  - `MessageAnalysisResult`から`appearances`フィールドを削除
  - `UserWordAnalysisResult`から`appearances`フィールドを削除（存在する場合）
  - `UserMessageAnalysisResult`から`appearances`フィールドを削除（存在する場合）
- [x] `app/services/word_counter.py`の修正
  - `appearances`収集処理をコメントアウト
  - 将来の時系列解析のために処理ロジックは保持
  - コメントで削除理由と今後の拡張方法を明記
- [x] `app/services/analyzer.py`の修正
  - `appearances`に関する処理をコメントアウト
  - レスポンス整形時に`appearances`を含めない
  - 将来の拡張のためのコメントを追加
- [x] テストコードの修正
  - `tests/unit/test_models.py`: `appearances`の検証を削除
  - `tests/unit/test_word_counter.py`: `appearances`のテストをコメントアウト（処理は残すため）
  - `tests/unit/test_analyzer.py`: `appearances`のアサーションを削除
  - `tests/integration/test_api.py`: APIレスポンスの`appearances`チェックを削除
  - `tests/e2e/test_real_data.py`: `appearances`の検証を削除
- [x] ドキュメントの更新
  - `README.md`: レスポンス例から`appearances`を削除
  - `doc/SPEC.md`: 
    - セクション3.2.1のレスポンス例を更新
    - Issue#01の完了チェック

**将来の拡張に向けた方針**:
- 時系列解析、ユーザー行動分析などの機能を追加する際は、以下のアプローチを検討：
  1. **専用エンドポイントの追加**: `/api/v1/analyze/timeline`など、詳細データが必要な場合のみ呼び出す
  2. **ページネーション**: `appearances`を返す場合は、limit/offsetで分割取得
  3. **データベース保存**: 解析結果をDB保存し、必要に応じてクエリで取得
  4. **サマリーデータのみ返却**: 日別集計、月別集計など、集約済みデータを返す
- コメントアウトした処理は、上記実装時の参考コードとして活用

**影響範囲**:
- レスポンスモデル: 3ファイル
- サービス層: 2ファイル
- テストコード: 5ファイル
- ドキュメント: 2ファイル

**期待される効果**:
- レスポンスサイズ: 約4MB → 約0.05MB（約80倍削減）
- モバイルブラウザでのセッションストレージ保存が可能に
- ネットワーク転送速度の向上
- フロントエンドのメモリ使用量削減
- ユーザー体験の改善

**テスト計画**:
- [x] 全ての単体テストがパスすること
- [x] 全ての統合テストがパスすること
- [x] E2Eテストで実際のレスポンスサイズを確認
  - 2025年分（41,539メッセージ）: 50KB以下を目標
  - 全期間（272,878メッセージ）: 200KB以下を目標
- [x] デプロイ後、スマホでの動作確認
  - セッションストレージへの保存が成功すること
  - 解析結果の表示が正しく動作すること

**依存**: PR#9（E2Eテストと最終調整）

**完了**: ✅

---

#### Issue#02: 改行されたメッセージを正しくカウントする

**目的**: LINEトーク履歴における改行を含むメッセージを正しく解析・集計する

**背景**:
- LINEのトーク履歴エクスポート機能では、改行を含むメッセージはダブルクォーテーション（`"`）で囲われる仕様となっている
- 現在のパーサーは1行単位でメッセージを解析するため、以下の問題が発生：
  1. 改行を含むメッセージの1行目のみを解析してしまう
  2. メッセージ本文に`"`が含まれた状態でカウントされる
  3. 2行目以降が無視され、単語解析の対象外となる
  4. 不完全なメッセージとして集計され、流行語の精度が低下する

**問題の具体例**:

元のメッセージ:
```
今日は天気が良かったです
明日も晴れるといいな
```

トーク履歴ファイルでの記載:
```
23:01	山田太郎	"今日は天気が良かったです
明日も晴れるといいな"
```

現在の動作（問題あり）:
- 解析されるメッセージ: `"今日は天気が良かったです`
- 2行目の`明日も晴れるといいな"`は無視される
- 単語解析も1行目のみが対象となる

期待される動作:
- 解析されるメッセージ: `今日は天気が良かったです\n明日も晴れるといいな`
- 改行を含む全文が1つのメッセージとして扱われる
- 単語解析も全行が対象となる

**仕様**:

1. **改行メッセージの検出条件**
   - メッセージの1文字目が`"`である
   - かつ、メッセージの末尾が`"`ではない（1行で完結していない）
   - かつ、次の行がメッセージ行の形式（`HH:MM\tユーザー名\t`）ではない

2. **複数行メッセージの結合処理**
   - 改行メッセージと判定された場合、次の行以降を順次読み込む
   - 末尾が`"`の行が出現するまで、または次のメッセージ行が出現するまで結合
   - 結合時は改行文字（`\n`）を保持する
   - 先頭と末尾の`"`を除去して、本来のメッセージ内容のみを抽出

3. **エッジケース**
   - メッセージ内に`"`が含まれる場合（エスケープなし）: 検証必要
   - 改行のみのメッセージ: 空メッセージとして除外
   - ファイル末尾で閉じ`"`がない場合: エラーハンドリング
   - 連続する複数の改行: 改行文字として保持

4. **単語解析の対応**
   - 形態素解析は改行を含む全文に対して実行
   - 改行文字は単語の区切りとして扱う（MeCabが自動的に処理）
   - メッセージ全文カウントも改行を含む全文を対象とする

**修正対象ファイル**:

1. **`app/services/parser.py`** ⭐ 主要修正
   - `_parse_message_line()`メソッドの拡張
     - 改行メッセージの検出ロジック追加
     - 複数行読み込み処理の実装
   - `parse()`メソッドの修正
     - 行単位ループから、状態管理を含むループへ変更
     - イテレータパターンの導入または行バッファリング
   - 新規メソッドの追加
     - `_is_multiline_message_start()`メソッド: 改行メッセージ開始判定
     - `_is_message_line()`メソッド: 次の行がメッセージ行かどうかの判定
     - `_read_multiline_message()`メソッド: 複数行の読み込みと結合

2. **`tests/fixtures/sample_talk.txt`**
   - 改行メッセージのテストケース追加
     - 標準的な2行メッセージ
     - 3行以上のメッセージ
     - 改行のみのメッセージ
     - 末尾に`"`がないケース（エラーケース）

3. **`tests/unit/test_parser.py`**
   - 改行メッセージのテストケース追加（7件）
     - 正常系: 2行メッセージの解析
     - 正常系: 3行以上のメッセージの解析
     - 正常系: 連続する改行の処理
     - 正常系: メッセージ内容に`"`が含まれるケース
     - 異常系: 閉じ`"`がない場合
     - 境界値: 改行のみのメッセージ
     - 統合: 通常メッセージと改行メッセージが混在

4. **`tests/integration/test_api.py`**（オプション）
   - 改行メッセージを含むトーク履歴での解析確認
   - レスポンスに改行メッセージ由来の単語が含まれることの確認

5. **`tests/e2e/test_real_data.py`**
   - 実際のsample.txtでの改行メッセージ処理確認
   - 改行メッセージの件数レポート追加

**実装方針**:

##### Phase 1: パーサーのリファクタリング
- `parse()`メソッドを行リストベースに変更（インデックスアクセス可能に）
- または、イテレータをpeekableにラップ

##### Phase 2: 改行メッセージ検出機能の実装
- `_is_multiline_message_start()`の実装
- 正規表現による判定ロジック

##### Phase 3: 複数行読み込み機能の実装
- `_read_multiline_message()`の実装
- 行結合とクォート除去

##### Phase 4: テストの実装
- 単体テスト、統合テスト、E2Eテストの順に実装

##### Phase 5: 実データでの検証
- sample.txtでの動作確認
- パフォーマンス影響の確認（目標: 10秒以内維持）

**期待される効果**:
- 改行を含むメッセージが正しくカウントされる
- 流行語の精度向上（2行目以降の単語も集計される）
- メッセージ全文カウントの精度向上
- ユーザーが実際に送信した内容に忠実な解析結果

**パフォーマンス考慮**:
- ファイル全体を事前にリストに読み込む場合、メモリ使用量が増加
  - 50MBファイル（約27万メッセージ）でも影響は軽微（数十MB増）
  - 現在のMeCab処理のほうがボトルネック（2.5秒程度）
- 改行メッセージの出現頻度は低い（全メッセージの1%未満と推定）
- 追加処理による影響は0.1秒未満と予想

**テスト計画**:
- [x] 単体テスト: `tests/unit/test_parser.py`
  - [x] 2行の改行メッセージの解析
  - [x] 3行以上の改行メッセージの解析
  - [x] 連続する改行（`\n\n`）の処理
  - [x] メッセージ内に`"`が含まれるケース
  - [x] 閉じ`"`がない異常ケース
  - [x] 改行のみのメッセージ（空メッセージ扱い）
  - [x] 通常メッセージと改行メッセージの混在
- [x] 統合テスト: `tests/integration/test_api.py`
  - [x] 改行メッセージを含むファイルの解析
  - [x] 改行メッセージ由来の単語が結果に含まれることの確認
- [x] E2Eテスト: `tests/e2e/test_real_data.py`
  - [x] sample.txtでの改行メッセージ処理確認
  - [x] パフォーマンス影響の測定（10秒以内を維持）
  - [x] 改行メッセージ件数のレポート

**実測値**:
- 改行メッセージ数：6,339件（全272,878メッセージ中の2.32%）
- 最大改行数：8行
- パフォーマンス影響：なし（想定通り）

**依存**: なし（独立したIssueとして実装可能）

**優先度**: 高（流行語精度に直接影響）

**完了**: ✅

---

#### Issue#03: 単語カウント方法の改善

**目的**: 流行語ランキングの精度向上

**実施した改善（5項目）**:

1. **連続名詞の結合**
   - 固有名詞や複合語を1単語として認識
   - 例: 「機動」+「戦士」+「ガンダム」→「機動戦士ガンダム」
   - 効果: 「dアニメストア」「シャニマス」「モンスト」等を正しく抽出

2. **URL除外**
   - メッセージ本文からURLを自動除外（改行保持）
   - 正規表現: `r"https?://\S+"`
   - 効果: URL関連単語が0件に（以前はドメイン名等がノイズとして混入）

3. **MeCab辞書の明示化**
   - neologd辞書を明示的に指定: `MeCab.Tagger("-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd")`
   - 効果: 記号の正しい認識（「！#」→ 記号-一般）、記号単語が0件に

4. **絵文字のテキスト変換防止**
   - neologd辞書が絵文字を日本語に変換する問題を解決（例: 😭 → 「泣き顔」）
   - `_contains_emoji()`関数で判定し、表層形を維持
   - 制御文字（バリエーションセレクタU+FE0F等）も除外
   - 効果: 😭が「😭」として1024回正しくカウント

5. **全名詞で表層形使用**
   - neologd辞書の正規化を回避（「アオ」→「A-O」問題の解決）
   - 理論的根拠: 名詞は活用しないため、基本形と表層形が実質同じ
   - 効果: 「アオ」が「アオ」として111回カウント（ユーザーが実際に使った言葉を反映）

**主な修正ファイル**:
- `app/services/morphological.py`: 改善1, 3, 4, 5
- `app/services/parser.py`: 改善2
- テスト追加: 30件（形態素解析+27件、パーサー+3件）

**テスト結果**:
- 全157テスト成功（146→157に増加）
- URL関連単語: 0件
- 記号単語: 0件
- 制御文字: 0件

**パフォーマンス**:
- 解析時間: 3.48秒（2025年分41,142メッセージ）
- 5項目の改善で+1.00秒の増加（2.48→3.48秒）
- 目標10秒以内を維持

**詳細ドキュメント**: [doc/ISSUE/ISSUE03.md](ISSUE/ISSUE03.md)

**依存**: なし（独立したIssueとして実装）

**完了**: ✅ 

---

#### Issue#04: ひらがな、カタカナの1文字単語を除外する

**目的**: 流行語ランキングの品質向上（意味のない1文字単語の除外）

**背景**:
- 実データでの解析結果で「あ」「え」「ア」「ン」など、1文字のひらがな・カタカナが流行語ランキングに出現
- これらは会話の相槌や感嘆詞として使われるが、流行語としては面白みに欠ける
- 一方、「草」「愛」「w」「😭」などの漢字1文字、アルファベット1文字、絵文字は流行語として意味があるため除外すべきではない
- 現在の`min_length`フィルタ（2文字以上）では、すべての1文字単語が除外されてしまう問題がある

**問題の具体例**:
- 除外すべき1文字単語:
  - ひらがな: 「あ」「え」「お」「ん」
  - カタカナ: 「ア」「エ」「ン」「ー」
- 除外すべきでない1文字単語:
  - 漢字: 「草」「愛」「神」「嘘」
  - アルファベット: 「w」「W」「a」「A」
  - 絵文字: 「😭」「😂」「🙏」
  - 記号: 「！」「？」「♪」

**修正方針**:
- `MorphologicalAnalyzer`の`_filter_by_length()`メソッドを拡張
- 1文字単語の場合、文字種別でフィルタリング
- ひらがな、カタカナのみで構成される1文字単語を除外
- 漢字、アルファベット、絵文字、記号を含む1文字単語は許可

**実装詳細**:

##### 1. 文字種別判定関数の追加

`app/services/morphological.py`に新規関数を追加：

```python
def _is_single_kana(text: str) -> bool:
    """1文字のひらがな、カタカナかどうかを判定
    
    Args:
        text (str): 判定対象の文字列
    
    Returns:
        bool: 1文字のひらがな、カタカナならTrue
    """
    # 1文字でない場合はFalse
    if len(text) != 1:
        return False
    
    char = text[0]
    code_point = ord(char)
    
    # ひらがな（U+3040-U+309F）
    if 0x3040 <= code_point <= 0x309F:
        return True
    
    # カタカナ（U+30A0-U+30FF）
    if 0x30A0 <= code_point <= 0x30FF:
        return True
    
    # 半角カタカナ（U+FF65-U+FF9F）
    if 0xFF65 <= code_point <= 0xFF9F:
        return True
    
    return False
```

##### 2. `_filter_by_length()`メソッドの修正

```python
def _filter_by_length(self, word: Word) -> bool:
    """文字数でフィルタリング
    
    1文字単語の場合、ひらがな・カタカナを除外
    漢字、アルファベット、絵文字、記号を含む1文字単語は許可
    
    Args:
        word (Word): チェック対象の単語
    
    Returns:
        bool: 条件を満たす場合True
    """
    word_length = len(word.surface)
    
    # min_length未満は除外
    if word_length < self.min_length:
        return False
    
    # 1文字の場合、追加チェック
    if word_length == 1:
        # ひらがな、カタカナのみの場合は除外
        if _is_single_kana(word.surface):
            return False
        # 上記以外（漢字、アルファベット、絵文字、記号など）は許可
    
    return True
```

**修正対象ファイル**:

1. **`app/services/morphological.py`**
   - `_is_single_kana()`関数を追加（モジュールレベル）
   - `_filter_by_length()`メソッドを修正

2. **`tests/unit/test_morphological.py`**
   - `_is_single_kana()`のテストケース追加（13件）
     - ひらがな1文字: 「あ」「ん」
     - カタカナ1文字: 「ア」「ン」「ー」
     - 半角カタカナ1文字: 「ｱ」
     - 漢字1文字（除外されない）: 「草」「愛」
     - アルファベット1文字（除外されない）: 「w」「W」
     - 絵文字1文字（除外されない）: 「😭」
     - 記号1文字（除外されない）: 「！」
     - 2文字以上（除外されない）: 「あい」
   - `_filter_by_length()`のテストケース追加（9件）
     - 1文字ひらがな除外: 「あ」
     - 1文字カタカナ除外: 「ア」
     - 1文字半角カタカナ除外: 「ｱ」
     - 1文字漢字許可: 「草」
     - 1文字アルファベット許可: 「w」「W」
     - 1文字絵文字許可: 「😭」
     - 1文字記号許可: 「！」
     - 2文字以上許可: 「あい」
   - `analyze()`の統合テスト追加（2件）
     - 1文字単語混在テキストの解析
     - 結果に漢字1文字・アルファベット1文字が含まれ、ひらがな1文字が含まれないことを確認

3. **`tests/integration/test_api.py`**（オプション）
   - 1文字単語を含むトーク履歴での解析確認
   - レスポンスに漢字1文字・アルファベット1文字が含まれ、ひらがな1文字が含まれないことを確認

4. **`tests/e2e/test_real_data.py`**
   - sample.txtでの1文字単語フィルタリング確認
   - 除外された1文字単語の件数レポート
   - 残された1文字単語（漢字、アルファベット、絵文字）の件数レポート

**テスト計画**:
- [x] 単体テスト: `tests/unit/test_morphological.py`
  - [x] `_is_single_kana()`の動作確認（13件）
  - [x] `_filter_by_length()`の動作確認（9件）
  - [x] `analyze()`の統合テスト（2件）
- [ ] 統合テスト: `tests/integration/test_api.py`（オプション）
  - [ ] 1文字単語フィルタリングの確認（1件）
- [x] E2Eテスト: `tests/e2e/test_real_data.py`
  - [x] sample.txtでの1文字単語フィルタリング確認（確認済み）
  - [x] 統計情報のレポート（確認済み）

**実測値**:
- ひらがな1文字除外: 0件（正常に除外）
- カタカナ1文字除外: 0件（正常に除外）
- 漢字1文字許可: 正常に抽出（例: 「草」「愛」「神」）
- アルファベット1文字許可: 正常に抽出（例: 「w」「W」）
- テスト成功: 181/181件パス（新規24件追加）

**期待される効果**:
- ひらがな・カタカナなどの意味の薄い1文字は除外される
- 「草」「愛」などの意味のある漢字1文字は残る
- 「w」「W」などのアルファベット1文字も流行語として残る
- 絵文字や記号も1文字でも正しくカウントされる
- ユーザーが見て面白い流行語ランキングになる

**パフォーマンス考慮**:
- 文字種別判定は単純な範囲チェックのみ（O(1)）
- 1文字単語のみが対象のため、影響は軽微
- パフォーマンス影響: 0.01秒未満と予想

**エッジケース**:
- 制御文字（U+FE0F等）を含む絵文字は`len()`で2文字以上になるため、このフィルタの対象外（すでに正しく処理されている）
- 半角カタカナ（「ｱ」「ｲ」）も除外対象に含める
- 全角・半角アルファベットは除外対象外（流行語として許可）

**優先度**: 中（流行語の品質向上）

**完了**: ✅

---

#### Issue#05: 形容動詞語幹を連続名詞結合の対象に含める

**目的**: 人名の一部として使われる形容動詞語幹が正しく結合されるようにする

**背景**:
- Issue03で実装した連続名詞結合機能において、形容動詞語幹を結合対象外としていた
- 実データの解析で、人名の一部として使われる形容動詞語幹（例：「稀」）が他の名詞と結合されず、別々にカウントされる問題が発覚
- MeCabが人名「優稀」を「優（名詞-固有名詞）」+「稀（名詞-形容動詞語幹）」と解析
- 形容動詞語幹が`NON_COMBINABLE_NOUN_DETAILS`に含まれていたため、結合されない

**問題の発見:**
- 実データの流行語ランキングで以下が出現：
  - 2位: 稀 (27回)
  - 3位: 優 (24回)
- これらは人名として使用されており、本来は1つの単語として結合されるべき

**実装内容:**

1. **`app/services/morphological.py`の修正**
   - `COMBINABLE_NOUN_DETAILS`に「形容動詞語幹」を追加
   - `NON_COMBINABLE_NOUN_DETAILS`から「形容動詞語幹」を削除

2. **形容動詞の文法的性質を活用**
   - 形容動詞として使われる場合、必ず助動詞（「な」「だ」など）が後続する
   - 例：「綺麗な花」→ 綺麗/な/花（「な」が間に入るため結合されない）
   - 人名として使われる場合、直接連続する
   - 例：「優稀」→ 優/稀（連続するため結合される）

**修正対象ファイル**:

1. **`app/services/morphological.py`**: 6行修正
   - `COMBINABLE_NOUN_DETAILS`に「形容動詞語幹」を追加
   - `NON_COMBINABLE_NOUN_DETAILS`から「形容動詞語幹」を削除
   - コメント更新

2. **`tests/unit/test_morphological.py`**: 約80行追加
   - `TestKeiyoudoushiGokanCombination`クラスを追加（5テスト）
     - 人名「優稀さん」→ 「優稀」に結合
     - フルネーム「山田優稀」→ 「山田優稀」に結合
     - 形容動詞「綺麗な花」→ 結合されない（「な」が間に入る）
     - 形容動詞「元気な人」→ 結合されない
     - 助詞なし「静か部屋」→ 結合される

**テスト計画**:
- [x] 単体テスト: `tests/unit/test_morphological.py`
  - [x] 人名での結合確認（2件）
  - [x] 形容動詞での非結合確認（2件）
  - [x] 助詞なしでの結合確認（1件）
- [x] E2Eテスト: `tests/e2e/test_real_data.py`
  - [x] 実データで人名が正しく結合されることを確認

**実測値**:
- 修正前: 「稀」(27回)、「優」(24回)が別々にカウント
- 修正後: 「優稀」(24回)として正しく結合
- テスト成功: 162/162件パス（新規5件追加）
- パフォーマンス影響: 測定不能レベル（< 0.01秒）

**期待される効果**:
- 人名が1つの単語として正しくカウントされる
- 形容動詞として使う場合に不自然な結合が発生しない
- より正確な流行語ランキング

**依存**: Issue#03（連続名詞結合機能の実装）

**優先度**: 高（Issue03の改善・バグ修正）

**完了**: ✅

---

#### Issue#09: サンプルレスポンスの作成

**目的**: デモ・宣伝用のモックレスポンス機能の実装

**背景**:
- アプリケーションの宣伝やデモ時に、実際の解析結果画面を表示したい
- 実際のトーク履歴には個人情報が含まれるため、公開用途では使用できない
- モックデータを返す機能を実装し、デモ環境でリアルな動作を再現する
- 実際の解析と同様の遅延時間を設けることで、ユーザー体験を忠実に再現

**要件**:

1. **モック機能のトリガー**
   - 特殊な形式のファイルを受信した場合、実解析をスキップしてモックレスポンスを返す
   - トリガーファイル名: `__DEMO__.txt`
   - ファイルサイズ: 1KB未満（中身は空でも可）
   - クライアントからは通常のAPIコールと同じ方法で呼び出し可能

2. **遅延時間の設定**
   - 実際の解析と同様の時間がかかるように、意図的に遅延を設ける
   - デフォルト遅延時間: 3秒（約1000メッセージの解析時間を想定）
   - 環境変数`DEMO_RESPONSE_DELAY_SECONDS`で設定可能（デフォルト: 3）
   - `asyncio.sleep()`を使用して非同期処理を維持

3. **モックレスポンスの内容**

   **基本情報**:
   - `total_messages`: 1000
   - `total_users`: 3
   - `analysis_period`:
     - `start_date`: "2025-01-01"
     - `end_date`: "2025-12-31"

   **流行語ランキング（`morphological_analysis.top_words`）**: 50件
   - 2025年の流行語大賞からトップ30を選定（実際の流行語）
   - 残り20件は一般的な日常会話の単語
   - 出現回数はZipf分布に従う（1位: 150回、2位: 120回...）
   - 品詞情報も付与

   **流行メッセージランキング（`full_message_analysis.top_messages`）**: 30件
   - 口癖やよく使われるフレーズ
   - 例: 「それな」「わかる」「草」「まじで」「やばい」など
   - 出現回数はZipf分布に従う（1位: 80回、2位: 65回...）

   **ユーザー別統計（`user_analysis`）**: 3ユーザー
   - ユーザー名: 「太郎」「花子」「次郎」
   - 各ユーザーのメッセージ数: 350, 400, 250（合計1000）
   - 各ユーザーの流行語・流行メッセージトップ10

4. **流行語リストの作成**

   **参考資料**: `/app/talk/流行語大賞2025.txt`から抽出

   **トップ50の流行語（案）**:
   
   1. エッホエッホ (150回)
   2. チャッピー (120回)
   3. ミャクミャク (105回)
   4. ぬい活 (92回)
   5. ビジュイイじゃん (85回)
   6. ほいたらね (78回)
   7. オンカジ (72回)
   8. 麻辣湯 (68回)
   9. トランプ関税 (65回)
   10. 古古古米 (60回)
   11. 物価高 (58回)
   12. 二季 (55回)
   13. 平成女児 (52回)
   14. ラブブ (50回)
   15. 国宝 (48回)
   16. 教皇選挙 (46回)
   17. 企業風土 (44回)
   18. リカバリーウエア (42回)
   19. フリーランス保護法 (40回)
   20. 薬膳 (38回)
   21. チョコミントよりもあなた (36回)
   22. 緊急銃猟 (34回)
   23. クマ被害 (32回)
   24. 女性首相 (30回)
   25. おてつたび (28回)
   26. オールドメディア (26回)
   27. ひょうろく (24回)
   28. 卒業証書 (22回)
   29. 長袖 (20回)
   30. 戦後80年 (18回)
   31. ご飯 (50回) ※日常会話
   32. 仕事 (48回)
   33. 映画 (46回)
   34. 美味しい (44回)
   35. 楽しい (42回)
   36. 疲れた (40回)
   37. 今日 (38回)
   38. 明日 (36回)
   39. 昨日 (34回)
   40. 時間 (32回)
   41. 予定 (30回)
   42. 会議 (28回)
   43. ランチ (26回)
   44. 帰宅 (24回)
   45. 天気 (22回)
   46. 寒い (20回)
   47. 暑い (18回)
   48. 忙しい (16回)
   49. ありがとう (14回)
   50. よろしく (12回)

5. **流行メッセージリストの作成**

   **口癖・定型フレーズ30件（案）**:
   
   1. それな (80回)
   2. わかる (65回)
   3. 草 (58回)
   4. まじで (52回)
   5. やばい (48回)
   6. 確かに (44回)
   7. なるほど (40回)
   8. いいね (38回)
   9. おつかれ (36回)
   10. ありがとう (34回)
   11. そうなんだ (32回)
   12. うける (30回)
   13. わかりみ (28回)
   14. ほんとそれ (26回)
   15. 了解 (24回)
   16. おやすみ (22回)
   17. おはよう (20回)
   18. いってきます (18回)
   19. ただいま (16回)
   20. お疲れ様 (15回)
   21. すごい (14回)
   22. えぐい (13回)
   23. さすが (12回)
   24. かわいい (11回)
   25. 最高 (10回)
   26. 無理 (9回)
   27. マジか (8回)
   28. ウケる (7回)
   29. やばみ (6回)
   30. りょ (5回)

**実装対象ファイル**:

1. **`app/core/config.py`**
   - `DEMO_RESPONSE_DELAY_SECONDS`: デモレスポンスの遅延時間（デフォルト: 3秒）
   - `DEMO_TRIGGER_FILENAME`: トリガーファイル名（デフォルト: "__DEMO__.txt"）

2. **`app/data/demo_response.json`** （新規作成）
   - モックレスポンスのJSONデータを静的ファイルとして保存
   - 上記の流行語・流行メッセージを含む完全なレスポンス構造
   - 読み込み時にJSONパースして返却

3. **`app/services/demo_service.py`** （新規作成）
   - `DemoService`クラス
     - `is_demo_file(filename: str) -> bool`: デモファイル判定
     - `load_demo_response() -> dict`: モックレスポンスの読み込み
     - `async generate_demo_response(delay_seconds: float) -> dict`: 遅延付きレスポンス生成

4. **`app/api/v1/endpoints/analyze.py`**
   - アップロードされたファイル名をチェック
   - `__DEMO__.txt`の場合、`DemoService`を呼び出し
   - 実解析をスキップしてモックレスポンスを返却

**実装フロー**:

```python
# analyze.pyでの処理フロー
@router.post("/analyze")
async def analyze_talk_history(file: UploadFile = File(...)):
    # 1. デモファイル判定
    if demo_service.is_demo_file(file.filename):
        # 2. デモレスポンス生成（遅延付き）
        result = await demo_service.generate_demo_response(
            delay_seconds=settings.DEMO_RESPONSE_DELAY_SECONDS
        )
        return {"status": "success", "data": result}
    
    # 3. 通常の解析処理
    # ... 既存のコード ...
```

**Zipf分布の計算式**:

```python
# 1位を基準として、順位が下がるにつれて出現回数を減少
def calculate_count(rank: int, first_count: int) -> int:
    """Zipf分布に従って出現回数を計算
    
    Args:
        rank: 順位（1から開始）
        first_count: 1位の出現回数
    
    Returns:
        出現回数
    """
    return int(first_count / rank)
```

**テスト計画**:

1. **単体テスト**: `tests/unit/test_demo_service.py`（新規作成）
   - [x] `is_demo_file()`の動作確認
     - `__DEMO__.txt` → True
     - `test.txt` → False
     - 大文字小文字の判定
   - [x] `load_demo_response()`の動作確認
     - JSONファイルの読み込み
     - レスポンス構造の検証
     - 必須フィールドの存在確認
   - [x] `generate_demo_response()`の動作確認
     - 遅延時間の測定
     - レスポンス内容の検証

2. **統合テスト**: `tests/integration/test_api.py`（既存ファイルに追加）
   - [x] デモファイルでのAPI呼び出し
     - `__DEMO__.txt`をアップロード
     - ステータスコード200
     - レスポンス構造の検証
     - `total_messages`が1000
     - `total_users`が3
     - `top_words`が50件
     - `top_messages`が30件
   - [x] 通常ファイルとの動作の違いを確認
     - デモファイルは遅延あり、実解析なし
     - 通常ファイルは実解析あり
   - [x] 遅延時間の検証
     - 約3秒かかることを確認（±0.5秒の誤差許容）

3. **E2Eテスト**: `tests/e2e/test_demo.py`（新規作成）
   - [x] デモモードの完全なフロー
     - ファイルアップロード → レスポンス受信
     - フロントエンドで表示可能なデータ形式
     - レスポンスサイズの確認（10KB以下）
   - [x] 複数回呼び出しでの一貫性
     - 同じレスポンスが返ることを確認

**追加仕様**:

1. **ログ出力**
   - デモモードが使用された場合、ログに記録
   - `[DEMO MODE] Demo response returned for promotional purposes`

2. **環境変数での無効化**
   - `ENABLE_DEMO_MODE`: デモモード有効化フラグ（デフォルト: True）
   - 本番環境では`False`に設定してデモモードを無効化可能

3. **レスポンスヘッダー**
   - デモレスポンスの場合、カスタムヘッダーを追加
   - `X-Demo-Mode: true`
   - フロントエンドで「デモモード」表示が可能

**セキュリティ考慮**:
- デモファイルのファイルサイズ制限（1KB未満）
- 不正なファイル名パターンの除外
- レート制限の適用（通常のAPIと同じ）

**期待される効果**:
- 個人情報を含まない宣伝用のデモが可能
- リアルな動作を再現できる
- フロントエンドの開発・テストでも利用可能
- アプリの魅力を効果的に伝えられる

**依存**: なし（独立したIssueとして実装可能）

**優先度**: 中（宣伝・マーケティング用途）

**完了**: ✅

---

## 9. テスト戦略

### 9.1 テストレベル

| レベル | ツール | カバレッジ目標 |
|--------|--------|---------------|
| 単体テスト | pytest | 80%以上 |
| 統合テスト | pytest + TestClient | 主要フロー100% |
| E2Eテスト | 実データ | 実用性確認 |

### 9.2 テストデータ

- **最小データセット**: 1日分、2ユーザー、10メッセージ
- **標準データセット**: 1ヶ月分、3ユーザー、500メッセージ
- **大規模データセット**: 1年分、5ユーザー、10000メッセージ
- **異常データセット**: 不正フォーマット、特殊文字、空ファイル

### 9.3 CI/CD

- PRごとに自動テスト実行
- コードカバレッジレポート生成
- リントとフォーマットチェック
- 型チェック（mypy）

---

## 10. 非機能要件

### 10.1 パフォーマンス

- 1万メッセージの解析を10秒以内で完了
- 最大ファイルサイズ: 50MB
- 同時リクエスト数: 10（初期）

### 10.2 セキュリティ

- ファイルアップロードのバリデーション
- ファイルサイズ制限
- タイムアウト設定（30秒）
- 悪意のあるファイル内容の検証（オプション）

### 10.3 可用性

- ヘルスチェックエンドポイントの提供
- エラーログの出力
- グレースフルシャットダウン

---

## 11. 今後の拡張案

- データベースへの解析結果保存
- ユーザー認証機能
- 複数ファイルの一括解析
- 時系列グラフ用のデータ出力
- 感情分析の追加
- ユーザー別統計情報
- ワードクラウド用データ生成

---

## 12. 参考資料

- [FastAPI公式ドキュメント](https://fastapi.tiangolo.com/)
- [MeCab公式サイト](https://taku910.github.io/mecab/)
- [mecab-ipadic-neologd](https://github.com/neologd/mecab-ipadic-neologd)
- [PEP8スタイルガイド](https://pep8-ja.readthedocs.io/ja/latest/)
