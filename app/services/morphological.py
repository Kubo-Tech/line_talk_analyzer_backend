"""å½¢æ…‹ç´ è§£æã‚µãƒ¼ãƒ“ã‚¹

MeCabã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†è§£ã—ã€å“è©æƒ…å ±ã‚’ä»˜ä¸ã™ã‚‹
"""

import json
import re
from dataclasses import dataclass
from pathlib import Path

import MeCab


@dataclass
class Word:
    """å˜èªãƒ‡ãƒ¼ã‚¿

    Attributes:
        surface (str): è¡¨å±¤å½¢ï¼ˆå®Ÿéš›ã®å˜èªï¼‰
        base_form (str): åŸºæœ¬å½¢ï¼ˆåŸå½¢ï¼‰
        part_of_speech (str): å“è©
        part_of_speech_detail1 (str): å“è©ç´°åˆ†é¡1
        part_of_speech_detail2 (str): å“è©ç´°åˆ†é¡2
        part_of_speech_detail3 (str): å“è©ç´°åˆ†é¡3
    """

    surface: str
    base_form: str
    part_of_speech: str
    part_of_speech_detail1: str
    part_of_speech_detail2: str
    part_of_speech_detail3: str


def _contains_emoji(text: str) -> bool:
    """ãƒ†ã‚­ã‚¹ãƒˆã«çµµæ–‡å­—ãŒå«ã¾ã‚Œã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯

    Args:
        text (str): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        bool: çµµæ–‡å­—ãŒå«ã¾ã‚Œã‚‹å ´åˆTrue
    """
    # Unicodeçµµæ–‡å­—ã®ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯
    # ä¸»è¦ãªçµµæ–‡å­—ãƒ–ãƒ­ãƒƒã‚¯:
    # - U+1F600-U+1F64F: é¡”æ–‡å­—
    # - U+1F300-U+1F5FF: ãã®ä»–ã®è¨˜å·ã¨çµµæ–‡å­—
    # - U+1F680-U+1F6FF: äº¤é€šã¨åœ°å›³è¨˜å·
    # - U+2600-U+26FF: ãã®ä»–ã®è¨˜å·
    # - U+2700-U+27BF: è£…é£¾è¨˜å·
    # - U+FE00-U+FE0F: ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒ¬ã‚¯ã‚¿
    emoji_pattern = re.compile(
        "["
        "\U0001f600-\U0001f64f"  # é¡”æ–‡å­—
        "\U0001f300-\U0001f5ff"  # ãã®ä»–ã®è¨˜å·ã¨çµµæ–‡å­—
        "\U0001f680-\U0001f6ff"  # äº¤é€šã¨åœ°å›³è¨˜å·
        "\U0001f1e0-\U0001f1ff"  # å›½æ——
        "\U00002600-\U000026ff"  # ãã®ä»–ã®è¨˜å·
        "\U00002700-\U000027bf"  # è£…é£¾è¨˜å·
        "\U0001f900-\U0001f9ff"  # è£œåŠ©çµµæ–‡å­—
        "\U0001fa00-\U0001fa6f"  # æ‹¡å¼µçµµæ–‡å­—
        "\U00002300-\U000023ff"  # ãã®ä»–ã®æŠ€è¡“è¨˜å·
        "\U0000fe00-\U0000fe0f"  # ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒ¬ã‚¯ã‚¿
        "]+"
    )
    return bool(emoji_pattern.search(text))


class MorphologicalAnalyzer:
    """å½¢æ…‹ç´ è§£æå™¨

    MeCabã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã—ã€å˜èªã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹
    """

    # æŠ½å‡ºå¯¾è±¡ã®å“è©ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    DEFAULT_TARGET_POS = {
        "åè©",  # åè©å…¨èˆ¬
        "å½¢å®¹è©",  # å½¢å®¹è©å…¨èˆ¬
        "æ„Ÿå‹•è©",  # æ„Ÿå‹•è©
    }

    # é™¤å¤–ã™ã‚‹åè©ã®ç´°åˆ†é¡
    EXCLUDED_NOUN_DETAILS = {
        "éè‡ªç«‹",  # éè‡ªç«‹åè©ï¼ˆã€Œã“ã¨ã€ã€Œã‚‚ã®ã€ãªã©ï¼‰
        "ä»£åè©",  # ä»£åè©ï¼ˆã€Œã“ã‚Œã€ã€Œã‚ã‚Œã€ãªã©ï¼‰
        "æ•°",  # æ•°è©
    }

    # é™¤å¤–ã™ã‚‹å½¢å®¹è©ã®ç´°åˆ†é¡
    EXCLUDED_ADJ_DETAILS = {
        "éè‡ªç«‹",  # éè‡ªç«‹å½¢å®¹è©
        "æ¥å°¾",  # æ¥å°¾è¾
    }

    # é€£ç¶šåè©çµåˆã®å¯¾è±¡ã¨ãªã‚‹åè©ã®ç´°åˆ†é¡
    COMBINABLE_NOUN_DETAILS = {
        "ä¸€èˆ¬",  # ä¸€èˆ¬åè©ï¼ˆä¾‹: ã€Œãƒ—ãƒ©ãƒ¢ãƒ‡ãƒ«ã€ã€Œã‚¢ãƒ‹ãƒ¡ã€ï¼‰
        "å›ºæœ‰åè©",  # å›ºæœ‰åè©ï¼ˆä¾‹: ã€Œã‚¬ãƒ³ãƒ€ãƒ ã€ã€Œæ±äº¬ã€ï¼‰
        "ã‚µå¤‰æ¥ç¶š",  # ã‚µå¤‰æ¥ç¶šï¼ˆä¾‹: ã€Œã‚¹ãƒˆã‚¢ã€ã€Œã‚»ãƒ³ã‚¿ãƒ¼ã€ï¼‰ â€»å›ºæœ‰åè©ã®ä¸€éƒ¨ã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„
    }

    # é€£ç¶šåè©çµåˆã‹ã‚‰é™¤å¤–ã™ã‚‹åè©ã®ç´°åˆ†é¡
    NON_COMBINABLE_NOUN_DETAILS = {
        "éè‡ªç«‹",  # éè‡ªç«‹åè©ï¼ˆã€Œã‚‚ã®ã€ã€Œã“ã¨ã€ãªã©ï¼‰
        "ä»£åè©",  # ä»£åè©ï¼ˆã€Œã“ã‚Œã€ã€Œãã‚Œã€ãªã©ï¼‰
        "æ•°",  # æ•°è©ï¼ˆã€Œ1ã€ã€Œ2ã€ã€Œåã€ãªã©ï¼‰
        "æ¥å°¾",  # æ¥å°¾è¾ï¼ˆã€Œã•ã‚“ã€ã€Œå††ã€ã€Œå€‹ã€ãªã©ï¼‰
        "å½¢å®¹å‹•è©èªå¹¹",  # å½¢å®¹å‹•è©èªå¹¹ï¼ˆã€Œç¶ºéº—ã€ã€Œå…ƒæ°—ã€ãªã©ï¼‰
    }

    def __init__(
        self, min_length: int = 1, exclude_parts: set[str] | list[str] | None = None
    ) -> None:
        """å½¢æ…‹ç´ è§£æå™¨ã®åˆæœŸåŒ–

        Args:
            min_length (int): æœ€å°å˜èªé•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            exclude_parts (set[str] | list[str] | None): é™¤å¤–ã™ã‚‹å“è©ã®ãƒªã‚¹ãƒˆã¾ãŸã¯ã‚»ãƒƒãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Noneï¼‰

        Raises:
            RuntimeError: MeCabã®åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        try:
            # neologdè¾æ›¸ã‚’ä½¿ç”¨ï¼ˆæ–°èªãƒ»å›ºæœ‰åè©ã®èªè­˜ç²¾åº¦å‘ä¸Šï¼‰
            self.tagger = MeCab.Tagger(
                "-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd"
            )
        except Exception as e:
            raise RuntimeError(f"MeCabã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}") from e

        self.min_length = min_length
        self.exclude_parts = set(exclude_parts) if exclude_parts else set()

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿
        self.stop_words = self._load_stop_words()

    def analyze(self, text: str) -> list[Word]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æ

        Args:
            text (str): è§£æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            list[Word]: è§£æã•ã‚ŒãŸå˜èªã®ãƒªã‚¹ãƒˆ

        Raises:
            RuntimeError: å½¢æ…‹ç´ è§£æã«å¤±æ•—ã—ãŸå ´åˆ
        """
        if not text or not text.strip():
            return []

        # MeCabã§è§£æ
        try:
            node = self.tagger.parseToNode(text)
        except Exception as e:
            raise RuntimeError(f"å½¢æ…‹ç´ è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}") from e

        # å…¨ã¦ã®å½¢æ…‹ç´ ã‚’ä¸€æ—¦ãƒªã‚¹ãƒˆã«æ ¼ç´ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ï¼‰
        all_morphemes: list[Word] = []

        while node:
            # EOSãƒãƒ¼ãƒ‰ï¼ˆæ–‡æœ«ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            if node.surface:
                features = node.feature.split(",")

                # å“è©æƒ…å ±ã‚’å–å¾—ï¼ˆè¶³ã‚Šãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã§åŸ‹ã‚ã‚‹ï¼‰
                pos = features[0] if len(features) > 0 else ""
                pos_detail1 = features[1] if len(features) > 1 else ""
                pos_detail2 = features[2] if len(features) > 2 else ""
                pos_detail3 = features[3] if len(features) > 3 else ""
                base_form = features[6] if len(features) > 6 else node.surface

                # çµµæ–‡å­—ã®å ´åˆã¯åŸºæœ¬å½¢ã§ã¯ãªãè¡¨å±¤å½¢ï¼ˆçµµæ–‡å­—ãã®ã¾ã¾ï¼‰ã‚’ä½¿ç”¨
                # neologdè¾æ›¸ã¯çµµæ–‡å­—ã‚’æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ãŸã‚
                # ä¾‹: ğŸ˜­ -> ã€Œå¤§æ³£ãã€ã€ğŸ˜‚ -> ã€Œå¬‰ã—æ¶™ã€
                if _contains_emoji(node.surface):
                    base_form = node.surface

                word = Word(
                    surface=node.surface,
                    base_form=base_form,
                    part_of_speech=pos,
                    part_of_speech_detail1=pos_detail1,
                    part_of_speech_detail2=pos_detail2,
                    part_of_speech_detail3=pos_detail3,
                )

                all_morphemes.append(word)

            node = node.next

        # é€£ç¶šã™ã‚‹åè©ã‚’çµåˆ
        combined_morphemes = self._combine_consecutive_nouns(all_morphemes)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦æœ€çµ‚çµæœã‚’ä½œæˆ
        words: list[Word] = []
        for word in combined_morphemes:
            if self._should_include(word):
                words.append(word)

        return words

    def _load_stop_words(self) -> set[str]:
        """ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€

        Returns:
            set[str]: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆ
        """
        stopwords_path = Path(__file__).parent.parent / "data" / "stopwords.json"

        try:
            with open(stopwords_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data.get("stop_words", []))
        except FileNotFoundError:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
            return set()
        except json.JSONDecodeError as e:
            # JSONè§£æã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è­¦å‘Šã—ã¦ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
            print(f"è­¦å‘Š: stopwords.jsonã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return set()

    def _should_include(self, word: Word) -> bool:
        """å˜èªã‚’çµæœã«å«ã‚ã‚‹ã¹ãã‹ãƒã‚§ãƒƒã‚¯

        Args:
            word (Word): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®å˜èª

        Returns:
            bool: å«ã‚ã‚‹ã¹ããªã‚‰True
        """
        # æœ€å°æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯
        if not self._filter_by_length(word):
            return False

        # å“è©ãƒã‚§ãƒƒã‚¯
        if not self._filter_by_pos(word):
            return False

        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬å½¢ã§åˆ¤å®šï¼‰
        if word.base_form in self.stop_words:
            return False

        return True

    def _filter_by_length(self, word: Word) -> bool:
        """æ–‡å­—æ•°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

        Args:
            word (Word): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®å˜èª

        Returns:
            bool: æ¡ä»¶ã‚’æº€ãŸã™å ´åˆTrue
        """
        return len(word.surface) >= self.min_length

    def _filter_by_pos(self, word: Word) -> bool:
        """å“è©ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

        Args:
            word (Word): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®å˜èª

        Returns:
            bool: æ¡ä»¶ã‚’æº€ãŸã™å ´åˆTrue
        """
        pos = word.part_of_speech
        pos_detail1 = word.part_of_speech_detail1

        # é™¤å¤–å“è©ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯é™¤å¤–
        if pos in self.exclude_parts:
            return False

        # çµµæ–‡å­—ã‚’å«ã‚€è¨˜å·ã¯ç‰¹åˆ¥ã«è¨±å¯ï¼ˆè¡¨å±¤å½¢ã«çµµæ–‡å­—ãŒå«ã¾ã‚Œã‚‹å ´åˆï¼‰
        if pos == "è¨˜å·" and _contains_emoji(word.surface):
            return True

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¯¾è±¡å“è©ã«å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯é™¤å¤–
        if pos not in self.DEFAULT_TARGET_POS:
            return False

        # åè©ã®ç´°åˆ†é¡ãƒã‚§ãƒƒã‚¯
        if pos == "åè©" and pos_detail1 in self.EXCLUDED_NOUN_DETAILS:
            return False

        # å½¢å®¹è©ã®ç´°åˆ†é¡ãƒã‚§ãƒƒã‚¯
        if pos == "å½¢å®¹è©" and pos_detail1 in self.EXCLUDED_ADJ_DETAILS:
            return False

        return True

    def _is_combinable_noun(self, word: Word) -> bool:
        """é€£ç¶šåè©çµåˆã®å¯¾è±¡ã¨ãªã‚‹åè©ã‹ã©ã†ã‹ã‚’åˆ¤å®š

        Args:
            word (Word): ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®å˜èª

        Returns:
            bool: çµåˆå¯¾è±¡ã®åè©ãªã‚‰True
        """
        # å“è©ãŒåè©ã§ãªã„å ´åˆã¯å¯¾è±¡å¤–
        if word.part_of_speech != "åè©":
            return False

        # é™¤å¤–ã™ã‚‹åè©ã®ç´°åˆ†é¡ã«è©²å½“ã™ã‚‹å ´åˆã¯å¯¾è±¡å¤–
        if word.part_of_speech_detail1 in self.NON_COMBINABLE_NOUN_DETAILS:
            return False

        # çµåˆå¯¾è±¡ã®åè©ã®ç´°åˆ†é¡ã«è©²å½“ã™ã‚‹å ´åˆã¯å¯¾è±¡
        if word.part_of_speech_detail1 in self.COMBINABLE_NOUN_DETAILS:
            return True

        return False

    def _combine_consecutive_nouns(self, words: list[Word]) -> list[Word]:
        """é€£ç¶šã™ã‚‹åè©ã‚’1ã¤ã®å˜èªã«çµåˆ

        Args:
            words (list[Word]): å½¢æ…‹ç´ è§£æçµæœã®å˜èªãƒªã‚¹ãƒˆ

        Returns:
            list[Word]: é€£ç¶šåè©ã‚’çµåˆã—ãŸå˜èªãƒªã‚¹ãƒˆ
        """
        if not words:
            return []

        combined_words: list[Word] = []
        i = 0

        while i < len(words):
            current_word = words[i]

            # çµåˆå¯èƒ½ãªåè©ã®å ´åˆã€é€£ç¶šã™ã‚‹åè©ã‚’æ¢ã™
            if self._is_combinable_noun(current_word):
                # é€£ç¶šã™ã‚‹åè©ã‚’åé›†
                noun_group = [current_word]
                j = i + 1

                while j < len(words) and self._is_combinable_noun(words[j]):
                    noun_group.append(words[j])
                    j += 1

                # 2ã¤ä»¥ä¸Šã®åè©ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆã¯çµåˆ
                if len(noun_group) > 1:
                    # è¡¨å±¤å½¢ã‚’é€£çµ
                    combined_surface = "".join(word.surface for word in noun_group)

                    # çµåˆã•ã‚ŒãŸå˜èªã‚’ä½œæˆï¼ˆåŸºæœ¬å½¢ã‚‚è¡¨å±¤å½¢ã¨åŒã˜ã«ã™ã‚‹ï¼‰
                    combined_word = Word(
                        surface=combined_surface,
                        base_form=combined_surface,
                        part_of_speech="åè©",
                        part_of_speech_detail1="ä¸€èˆ¬",
                        part_of_speech_detail2="*",
                        part_of_speech_detail3="*",
                    )
                    combined_words.append(combined_word)
                    i = j  # æ¬¡ã®å˜èªã¸
                else:
                    # 1ã¤ã ã‘ã®å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
                    combined_words.append(current_word)
                    i += 1
            else:
                # åè©ä»¥å¤–ã¯ãã®ã¾ã¾è¿½åŠ 
                combined_words.append(current_word)
                i += 1

        return combined_words
